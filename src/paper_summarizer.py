"""
è®ºæ–‡æ€»ç»“æ¨¡å— - ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹APIç”Ÿæˆè®ºæ–‡æ‘˜è¦
"""
import os
import json
import re
from typing import List, Dict, Any, Optional
from pathlib import Path
import requests
import time
from datetime import datetime
import pytz
from config.settings import LLM_CONFIG
from src.clustering import get_embeddings, cluster_papers, select_representative_papers

class ModelClient:
    """å…¼å®¹ OpenAI æ¥å£æ ¼å¼çš„ API å®¢æˆ·ç«¯ (é€‚é… DashScope/DeepSeek)"""
    
    def __init__(self, api_key: str, model: Optional[str] = None):
        self.api_key = api_key
        self.model = model or LLM_CONFIG['model']
        # è‡ªåŠ¨æ‹¼æ¥ chat/completions ç«¯ç‚¹
        base_url = LLM_CONFIG['api_url'].rstrip('/')
        self.api_url = f"{base_url}/chat/completions"
        self.timeout = LLM_CONFIG.get('timeout', 60)
        
    def _create_headers(self) -> Dict[str, str]:
        """åˆ›å»ºè¯·æ±‚å¤´"""
        return {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
    
    def _create_request_body(
        self, 
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºç¬¦åˆ OpenAI æ ¼å¼çš„è¯·æ±‚ä½“"""
        return {
            "model": self.model,
            "messages": messages,
            "temperature": temperature or LLM_CONFIG['temperature'],
            "max_tokens": max_tokens or LLM_CONFIG['max_output_tokens'],
            # DashScope/OpenAI é€šå¸¸ä½¿ç”¨ top_pï¼Œä¸ä¸€å®šéœ€è¦ top_kï¼Œè§†æ¨¡å‹è€Œå®š
            "top_p": LLM_CONFIG['top_p']
        }
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """å‘é€è¯·æ±‚å¹¶è·å–å›å¤"""
        headers = self._create_headers()
        data = self._create_request_body(messages, temperature, max_tokens)
        
        for attempt in range(LLM_CONFIG['retry_count']):
            try:
                # æ‰“å°è°ƒè¯•ä¿¡æ¯ (æ³¨æ„ä¸è¦æ‰“å°å®Œæ•´çš„ API Key)
                print(f"æ­£åœ¨è°ƒç”¨æ¨¡å‹: {self.model}...")
                
                response = requests.post(
                    self.api_url,
                    headers=headers,
                    json=data,
                    timeout=self.timeout
                )
                
                if response.status_code != 200:
                    raise Exception(f"API è°ƒç”¨å¤±è´¥ [{response.status_code}]: {response.text}")
                    
                result = response.json()
                
                # è§£æ OpenAI æ ¼å¼çš„å“åº”
                content = result["choices"][0]["message"]["content"]
                
                return {
                    "choices": [{
                        "message": {
                            "role": "assistant",
                            "content": content
                        },
                        "finish_reason": "stop"
                    }],
                    "usage": result.get("usage", {})
                }
                
            except requests.Timeout:
                print(f"è¯·æ±‚è¶…æ—¶ï¼ˆ{self.timeout}ç§’ï¼‰ï¼Œæ­£åœ¨é‡è¯• ({attempt + 1}/{LLM_CONFIG['retry_count']})...")
                time.sleep(LLM_CONFIG['retry_delay'])
            except Exception as e:
                print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
                if attempt == LLM_CONFIG['retry_count'] - 1:
                    raise
                time.sleep(LLM_CONFIG['retry_delay'])

class PaperSummarizer:
    def __init__(self, api_key: str, model: Optional[str] = None):
        self.client = ModelClient(api_key, model)
        # æ ¹æ® Token é™åˆ¶è°ƒæ•´æ‰¹å¤„ç†å¤§å°ï¼ŒDeepSeek/Qwen ä¸Šä¸‹æ–‡è¾ƒé•¿ï¼Œå¯ä»¥è®¾å¤§ä¸€ç‚¹
        self.max_papers_per_batch = 5 

    def _generate_batch_summaries(self, papers: List[Dict[str, Any]], start_index: int) -> str:
        """ä¸ºä¸€æ‰¹è®ºæ–‡ç”Ÿæˆæ€»ç»“"""
        batch_prompt = ""
        has_full_text = False
        
        for i, paper in enumerate(papers, start=start_index):
            # ä¼˜å…ˆä½¿ç”¨å…³é”®ç« èŠ‚ï¼ˆæ‘˜è¦ã€Introductionã€Related Workã€Conclusionï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ‘˜è¦
            paper_content = paper.get('full_text')
            if paper_content:
                has_full_text = True
                content_type = "æ‘˜è¦ã€ä»‹ç»ã€ç›¸å…³å·¥ä½œå’Œæ€»ç»“"
                # é™åˆ¶é•¿åº¦é¿å… token è¶…é™ï¼ˆä¿ç•™å‰ 20000 å­—ç¬¦ï¼Œé€šå¸¸è¶³å¤Ÿè¦†ç›–å…³é”®ç« èŠ‚ï¼‰
                if len(paper_content) > 20000:
                    paper_content = paper_content[:20000] + "\n\n[æ³¨ï¼šå†…å®¹å·²æˆªæ–­ï¼Œä»…æ˜¾ç¤ºå‰20000å­—ç¬¦]"
            else:
                content_type = "æ‘˜è¦"
                paper_content = paper.get('summary', 'æ— æ‘˜è¦')
            
            batch_prompt += f"""
è®ºæ–‡ {i}ï¼š
æ ‡é¢˜ï¼š{paper['title']}
ä½œè€…ï¼š{', '.join(paper['authors'])}
å‘å¸ƒæ—¥æœŸï¼š{paper['published'][:10]}
arXivé“¾æ¥ï¼š{paper['entry_id']}
{content_type}ï¼š
{paper_content}

"""
        
        content_desc = "æ‘˜è¦ã€ä»‹ç»ã€ç›¸å…³å·¥ä½œå’Œæ€»ç»“" if has_full_text else "æ‘˜è¦"
        # è®©æ¨¡å‹åªç”Ÿæˆç»“æ„åŒ–å†…å®¹ï¼ˆJSONæ ¼å¼ï¼‰ï¼Œæ ¼å¼ç”±ä»£ç æ§åˆ¶
        final_prompt = f"""ä½ æ˜¯ä¸€åè´Ÿè´£"é‡‘èé¢†åŸŸ AI åº”ç”¨ç ”ç©¶"çš„é«˜çº§æŠ€æœ¯è¯„å®¡ä¸“å®¶ã€‚

ã€ä»»åŠ¡ã€‘
å¯¹ç»™å®šçš„è®ºæ–‡è¿›è¡Œç­›é€‰åˆ¤æ–­ï¼Œè¯„ä¼°å…¶æ˜¯å¦å€¼å¾—è¿›å…¥"é‡ç‚¹ç²¾è¯» / å¤ç° / åº”ç”¨è¯„ä¼°"æ± ã€‚

ä½ çš„ç›®æ ‡ä¸æ˜¯åˆ¤æ–­è®ºæ–‡æ˜¯å¦å­¦æœ¯ä¸Šä¸¥è°¨ï¼Œè€Œæ˜¯åˆ¤æ–­è¯¥è®ºæ–‡æ˜¯å¦ï¼š
- å±äºæˆ‘å…³æ³¨çš„æŠ€æœ¯æ–¹å‘
- å…·æœ‰æ˜ç¡®çš„å·¥ç¨‹å®ç°æˆ–åº”ç”¨ä»·å€¼
- å…·å¤‡è¿ç§»æˆ–æ”¹é€ åˆ°é‡‘èåœºæ™¯çš„å¯è¡Œæ€§

ã€é‡ç‚¹å…³æ³¨çš„ç ”ç©¶æ–¹å‘ï¼ˆè‡³å°‘å‘½ä¸­å…¶ä¸€ï¼‰ã€‘

1. éŸ³é¢‘ç›¸å…³ï¼ˆSpeech / Audioï¼‰
   - è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³ç†è§£ã€éŸ³é¢‘ç”Ÿæˆ
   - éŸ³é¢‘ä¸æ–‡æœ¬/è§†è§‰/ç»“æ„åŒ–æ•°æ®çš„å¤šæ¨¡æ€èåˆ
   - éŸ³é¢‘åœ¨ä¸šåŠ¡åœºæ™¯ä¸­çš„åº”ç”¨ï¼ˆå¦‚é€šè¯åˆ†æã€é£æ§ã€å®¢æœã€åˆè§„ï¼‰

2. å¤šæ¨¡æ€ï¼ˆMultimodalï¼‰
   - æ–‡æœ¬-å›¾åƒ / æ–‡æœ¬-éŸ³é¢‘ / æ–‡æœ¬-ç»“æ„åŒ–æ•°æ®
   - å¤šæ¨¡æ€æ¨ç†ã€å¤šæ¨¡æ€å¯¹é½ã€å¤šæ¨¡æ€æ•°æ®æ„é€ 
   - å¤šæ¨¡æ€åœ¨çœŸå®ä¸šåŠ¡ä»»åŠ¡ä¸­çš„è½åœ°æ–¹æ³•

3. å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰
   - LLM çš„å·¥ç¨‹åŒ–ã€æ¨ç†èƒ½åŠ›ã€å¯¹é½ã€è¯„æµ‹ã€è®­ç»ƒç­–ç•¥
   - é¢å‘å…·ä½“ä»»åŠ¡çš„ LLM åº”ç”¨ï¼Œè€Œéæ³›æ³›ç†è®ºåˆ†æ
   - LLM + ä¸šåŠ¡ç³»ç»Ÿ / æ•°æ® / å·¥å…· çš„ç»„åˆæ–¹å¼

4. æ•°æ®åˆæˆï¼ˆData Synthesis / Synthetic Dataï¼‰
   - åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•
   - åˆæˆæ•°æ®åœ¨è®­ç»ƒã€è¯„æµ‹ã€å¯¹é½ä¸­çš„å®é™…ä½œç”¨
   - æœ‰å¯å¤ç°æˆ–å¯è¿ç§»çš„æ•°æ®ç”Ÿæˆæµç¨‹

5. æ™ºèƒ½ä½“ï¼ˆAgentï¼‰
   - LLM Agentã€Tool-using Agentã€å¤šæ­¥å†³ç­– Agent
   - é¢å‘ä»»åŠ¡æ‰§è¡Œã€è§„åˆ’ã€æœç´¢ã€å®¡æŸ¥ç­‰åº”ç”¨
   - æ˜ç¡®çš„ç³»ç»Ÿæ¶æ„ï¼Œè€ŒéæŠ½è±¡å®šä¹‰

6. MoEï¼ˆMixture of Expertsï¼‰
   - MoE åœ¨è®­ç»ƒ/æ¨ç†/ç³»ç»Ÿå±‚é¢çš„è®¾è®¡
   - ä¸“å®¶è·¯ç”±ã€è´Ÿè½½å‡è¡¡ã€æ¨ç†åŠ é€Ÿç­‰å·¥ç¨‹é—®é¢˜
   - ä¸å®é™…ä¸šåŠ¡è´Ÿè½½ç›¸å…³çš„ MoE åº”ç”¨

ã€éœ€è¦æ˜ç¡®è¿‡æ»¤æ‰çš„è®ºæ–‡ç±»å‹ã€‘

è¯·ç›´æ¥åˆ¤å®šä¸º"ä¸æ¨è"ï¼Œè‹¥è®ºæ–‡ä¸»è¦å±äºä»¥ä¸‹æƒ…å†µä¹‹ä¸€ï¼š
- çº¯ç†è®ºåˆ†æï¼ˆå¦‚ä»…æœ‰æ•°å­¦æ¨å¯¼ã€å¤æ‚å®šç†è¯æ˜ï¼‰
- æ–¹æ³•é«˜åº¦æŠ½è±¡ï¼Œç¼ºä¹æ˜ç¡®ä»»åŠ¡æˆ–ç³»ç»Ÿè½ç‚¹
- æ— å®éªŒï¼Œæˆ–å®éªŒä»…ä¸º toy task / äººå·¥åˆæˆå°ä»»åŠ¡
- åº”ç”¨åœºæ™¯ä¸é‡‘èã€ä¼ä¸šçº§ç³»ç»Ÿå‡ ä¹æ— å…³è”ï¼Œä¸”è¿ç§»æˆæœ¬æé«˜
- å®Œå…¨èšç„¦äºè¯æ˜æœ€ä¼˜æ€§ã€æ”¶æ•›æ€§ã€å¤æ‚åº¦ç•Œé™ï¼Œè€Œéå¯ç”¨ç³»ç»Ÿ

è¯·é˜…è¯»ä»¥ä¸‹{len(papers)}ç¯‡è®ºæ–‡çš„{content_desc}ï¼Œä¸ºæ¯ç¯‡è®ºæ–‡æå–å…³é”®ä¿¡æ¯ã€‚

è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¾“å‡ºï¼Œæ¯ç¯‡è®ºæ–‡ä¸€ä¸ªå¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- chinese_title: ä¸­æ–‡ç¿»è¯‘çš„æ ‡é¢˜
- keywords: 2-5ä¸ªå…³é”®è¯æ ‡ç­¾ï¼Œç”¨ä¸­æ–‡é€—å·åˆ†éš”ï¼ˆä¾‹å¦‚ï¼šRAGä¼˜åŒ–ã€å¤šæ¨¡æ€ã€Agentæ¶æ„ï¼‰
- core_pain_point: æ ¸å¿ƒç—›ç‚¹ï¼Œä¸€å¥è¯æ¦‚æ‹¬ç°æœ‰æŠ€æœ¯æœ‰ä»€ä¹ˆç¼ºé™·
- technical_innovation: æŠ€æœ¯åˆ›æ–°ï¼Œè¯¦ç»†æè¿°è®ºæ–‡Methodéƒ¨åˆ†çš„æ–¹æ³•ã€æŠ€æœ¯ã€ç®—æ³•å’Œæµç¨‹ã€‚è¦æ±‚ï¼š1) ä½¿ç”¨ç¼–å·åˆ—è¡¨æ ¼å¼ï¼ˆ1) 2) 3) ...ï¼‰ï¼Œæ¯ä¸ªç¼–å·æè¿°ä¸€ä¸ªå…·ä½“çš„æŠ€æœ¯ç‚¹ï¼›2) è¯¦ç»†è¯´æ˜æŠ€æœ¯æ–¹æ³•ã€ç®—æ³•åç§°ã€æ¶æ„è®¾è®¡ã€æ•°æ®å¤„ç†æµç¨‹ç­‰ï¼›3) å¦‚æœæ¶‰åŠæ•°æ®é›†ï¼Œè¯´æ˜æ•°æ®è§„æ¨¡å’Œç±»å‹ï¼ˆå¦‚ï¼š2739ä¸ªç¯å¢ƒã€11,270æ¡å¤šè½®å¯¹è¯æ•°æ®ï¼‰ï¼›4) å¦‚æœæ¶‰åŠå®éªŒï¼Œè¯´æ˜å…³é”®å®éªŒç»“æœï¼ˆå¦‚ï¼šåœ¨5ä¸ªæ¨ç†åŸºå‡†ä¸Šè¶…è¶Šé™æ€æ•°æ®é›†è®­ç»ƒï¼‰ï¼›5) ç”¨ç®€æ´çš„æŠ€æœ¯è¯­è¨€æè¿°ï¼Œé¿å…å­¦æœ¯å¥—è¯ï¼›6) æ§åˆ¶åœ¨200å­—ä»¥å†…ï¼Œç¡®ä¿ä¿¡æ¯å®Œæ•´
- application_value: åº”ç”¨ä»·å€¼ï¼Œè¯´æ˜è¿™é¡¹æŠ€æœ¯èƒ½å¸¦æ¥ä»€ä¹ˆï¼ˆé™åˆ¶åœ¨88å­—ä»¥å†…ï¼‰
- summary: è¯¦ç»†æ€»ç»“ï¼Œæ§åˆ¶åœ¨200å­—ä»¥å†…
- decision: æ¨èå†³ç­–ï¼Œåªèƒ½æ˜¯ä»¥ä¸‹ä¸‰ä¸ªé€‰é¡¹ä¹‹ä¸€ï¼š"æ¨è" æˆ– "è¾¹ç¼˜å¯çœ‹" æˆ– "ä¸æ¨è"
- decision_reason: å†³ç­–ç†ç”±ï¼Œç”¨ä¸è¶…è¿‡150å­—æ€»ç»“ä¸ºä»€ä¹ˆåšå‡ºè¯¥åˆ¤æ–­ï¼ˆéœ€æ˜ç¡®è¯´æ˜æ˜¯å¦å‘½ä¸­å…³æ³¨æ–¹å‘ã€å·¥ç¨‹ä»·å€¼ã€é‡‘èåœºæ™¯å¯è¡Œæ€§ï¼‰

è¦æ±‚ï¼š
1. **æ‹’ç»åºŸè¯**ï¼šæ‰€æœ‰æè¿°å¿…é¡»ç›´å‡»è¦å®³ï¼Œä¸è¦ä½¿ç”¨"æœ¬æ–‡æå‡ºäº†..."è¿™ç§å­¦æœ¯å¥—è¯ã€‚
2. **é€šä¿—æ˜“æ‡‚**ï¼šç”¨ä¸šç•Œé€šç”¨çš„æŠ€æœ¯è¯­è¨€ï¼Œè€Œä¸æ˜¯æ™¦æ¶©çš„æ•°å­¦æè¿°ã€‚
3. **æŠ€æœ¯åˆ›æ–°è¯¦ç»†åŒ–**ï¼štechnical_innovation å­—æ®µå¿…é¡»è¯¦ç»†æè¿°è®ºæ–‡Methodéƒ¨åˆ†çš„å†…å®¹ï¼Œä½¿ç”¨ç¼–å·åˆ—è¡¨æ ¼å¼ï¼Œæ¯ä¸ªæŠ€æœ¯ç‚¹éƒ½è¦è¯´æ˜å…·ä½“çš„æ–¹æ³•ã€ç®—æ³•ã€æµç¨‹ã€æ•°æ®è§„æ¨¡æˆ–å®éªŒç»“æœã€‚æ ¼å¼ç¤ºä¾‹ï¼š"1) ç¼–ç¨‹é—®é¢˜è‡ªåŠ¨è½¬åŒ–ä¸ºå¯éªŒè¯æ¨ç†ç¯å¢ƒï¼ˆ2739ä¸ªç¯å¢ƒï¼‰ï¼›2) åŠ¨æ€éš¾åº¦æ§åˆ¶å™¨ä¿æŒç›®æ ‡å‡†ç¡®ç‡ï¼›3) ç¯å¢ƒæ·˜æ±°æœºåˆ¶ç»´æŒå¤šæ ·æ€§ï¼›4) åœ¨5ä¸ªæ¨ç†åŸºå‡†ä¸Šè¶…è¶Šé™æ€æ•°æ®é›†è®­ç»ƒã€‚"
4. **ä¿æŒç®€çŸ­**ï¼šæ€»ç»“ç¯‡å¹…ä¸¥æ ¼æ§åˆ¶åœ¨ 200 å­—ä»¥å†…ï¼Œåº”ç”¨ä»·å€¼é™åˆ¶åœ¨88å­—ä»¥å†…ï¼ŒæŠ€æœ¯åˆ›æ–°é™åˆ¶åœ¨200å­—ä»¥å†…ï¼Œå†³ç­–ç†ç”±é™åˆ¶åœ¨150å­—ä»¥å†…ã€‚
5. **åªè¾“å‡º JSON**ï¼šä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ–é¢å¤–æ–‡å­—ï¼Œç¡®ä¿æ‰€æœ‰è¾“å‡ºç¬¦åˆä¸¥æ ¼çš„ JSON æ ¼å¼ã€‚
6. **ä¸¥æ ¼è½¬ä¹‰**ï¼šç¡®ä¿è¾“å‡ºçš„ JSON å­—ç¬¦ä¸²ä¸­ï¼Œæ‰€æœ‰å­—æ®µå€¼å†…çš„åŒå¼•å·å¿…é¡»ä½¿ç”¨åæ–œæ è½¬ä¹‰ï¼ˆå¦‚ \"å†…å®¹\"ï¼‰ï¼Œç¦æ­¢å‡ºç°åŸå§‹çš„æ¢è¡Œç¬¦ã€‚
7. **ç»“æ„å®Œæ•´**ï¼šè¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„ JSON æ•°ç»„ï¼Œç¡®ä¿æ¯ç¯‡è®ºæ–‡å¯¹è±¡ä¹‹é—´æœ‰é€—å·åˆ†éš”ã€‚
8. **å†³ç­–å‡†ç¡®**ï¼šdecision å­—æ®µåªèƒ½æ˜¯"æ¨è"ã€"è¾¹ç¼˜å¯çœ‹"ã€"ä¸æ¨è"ä¸‰è€…ä¹‹ä¸€ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
[
  {{
    "chinese_title": "ç¤ºä¾‹æ ‡é¢˜",
    "keywords": "å…³é”®è¯1ã€å…³é”®è¯2ã€å…³é”®è¯3",
    "core_pain_point": "ä¸€å¥è¯ç—›ç‚¹æè¿°",
    "application_value": "åº”ç”¨ä»·å€¼æè¿°",
    "summary": "é€šä¿—ç®€å•åœ°æ€»ç»“è¯¥è®ºæ–‡çš„å†…å®¹",
    "decision": "æ¨è",
    "decision_reason": "å‘½ä¸­LLMå·¥ç¨‹åŒ–æ–¹å‘ï¼Œæå‡ºçš„å‚æ•°é«˜æ•ˆæ–¹æ³•å…·æœ‰æ˜ç¡®å®ç°è·¯å¾„ï¼Œå¯è¿ç§»åˆ°é‡‘èæ¨¡å‹è®­ç»ƒåœºæ™¯é™ä½æˆæœ¬",
    "technical_innovation": "1) ç¼–ç¨‹é—®é¢˜è‡ªåŠ¨è½¬åŒ–ä¸ºå¯éªŒè¯æ¨ç†ç¯å¢ƒï¼ˆ2739ä¸ªç¯å¢ƒï¼‰ï¼›2) åŠ¨æ€éš¾åº¦æ§åˆ¶å™¨ä¿æŒç›®æ ‡å‡†ç¡®ç‡ï¼›3) ç¯å¢ƒæ·˜æ±°æœºåˆ¶ç»´æŒå¤šæ ·æ€§ï¼›4) åœ¨5ä¸ªæ¨ç†åŸºå‡†ä¸Šè¶…è¶Šé™æ€æ•°æ®é›†è®­ç»ƒ"
  }},
  ...
]

å¾…å¤„ç†è®ºæ–‡åˆ—è¡¨ï¼š
{batch_prompt}"""

        try:
            response = self.client.chat_completion([{
                "role": "user",
                "content": final_prompt
            }])
            content = response["choices"][0]["message"]["content"].strip()
            
            # è§£æ JSON å¹¶ç»„è£…æˆå›ºå®šæ ¼å¼
            return self._format_papers_from_json(content, papers, start_index)
        except Exception as e:
            print(f"æ‰¹å¤„ç†ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"**[æœ¬æ‰¹æ¬¡ç”Ÿæˆå¤±è´¥]** é”™è¯¯ä¿¡æ¯: {str(e)}"

    def _format_papers_from_json(self, json_content: str, papers: List[Dict[str, Any]], start_index: int):
        """ä» JSON å†…å®¹ç»„è£…æˆå›ºå®šæ ¼å¼çš„ Markdownï¼ŒåŒæ—¶è¿”å›ç»“æ„åŒ–æ•°æ®
        
        Returns:
            tuple: (formatted_text, paper_data_list) - æ ¼å¼åŒ–æ–‡æœ¬å’Œç»“æ„åŒ–æ•°æ®
        """
        try:
            # 1. åŸºç¡€æ¸…ç†
            json_content = json_content.strip()
            if json_content.startswith('```'):
                json_content = re.sub(r'^```(?:json)?\s*\n', '', json_content)
                json_content = re.sub(r'\n```\s*$', '', json_content)
        
        # === æ–°å¢ï¼šå¼ºåˆ¶æ¸…æ´—éæ³•å­—ç¬¦ ===
        # ç§»é™¤ JSON å­—ç¬¦ä¸²å€¼ä¸­å¯èƒ½å­˜åœ¨çš„æœªè½¬ä¹‰æ¢è¡Œç¬¦ï¼ˆå°†å…¶æ›¿æ¢ä¸ºç©ºæ ¼ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œåªå¤„ç†ç®€å•çš„æ ¼å¼é—®é¢˜
            json_content = json_content.replace('\n', ' ').replace('\r', '') 
        # æ¢å¤è¢«è¯¯åˆ çš„ç»“æ„åŒ–æ¢è¡Œï¼ˆå¯é€‰ï¼Œä¸ºäº†è§£ææ›´ç¨³å¥ï¼‰
            json_content = re.sub(r'}\s*{', '},{', json_content) 
        
        # 2. å°è¯•è§£æ
            try:
                paper_data_list = json.loads(json_content)
            except json.JSONDecodeError:
            # å®¹é”™ï¼šå¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œå°è¯•æå–æœ€å¤–å±‚çš„ [ ]
                match = re.search(r'\[.*\]', json_content, re.DOTALL)
                if match:
                    paper_data_list = json.loads(match.group(0))
                else:
                    raise
            
            # ç»„è£…æˆå›ºå®šæ ¼å¼
            formatted_papers = []
            enriched_papers = []  # ä¿å­˜ç»“æ„åŒ–æ•°æ®
            
            for i, (paper, paper_data) in enumerate(zip(papers, paper_data_list)):
                paper_num = start_index + i
                
                # è·å–å†³ç­–ä¿¡æ¯ï¼ˆä¸æ·»åŠ å›¾æ ‡ï¼‰
                decision = paper_data.get('decision', 'æœªè¯„ä¼°')
                
                formatted_paper = f"""## {paper_num}. {paper['title']}
- **ä¸­æ–‡æ ‡é¢˜**: {paper_data.get('chinese_title', '')}
- **Link**: {paper['entry_id']}
- **æ¨èå†³ç­–:** {decision}
- **å†³ç­–ç†ç”±:** {paper_data.get('decision_reason', '')}
- **å…³é”®è¯:** {paper_data.get('keywords', '')}
- **æ ¸å¿ƒç—›ç‚¹:** {paper_data.get('core_pain_point', '')}
- **åº”ç”¨ä»·å€¼:** {paper_data.get('application_value', '')}
- **æ€»ç»“:** {paper_data.get('summary', '')}
- **æŠ€æœ¯åˆ›æ–°:** {paper_data.get('technical_innovation', '')}

---"""
                formatted_papers.append(formatted_paper)
                
                # ä¿å­˜ç»“æ„åŒ–æ•°æ®ï¼ˆåˆå¹¶åŸå§‹è®ºæ–‡ä¿¡æ¯å’ŒLLMç”Ÿæˆçš„ä¿¡æ¯ï¼‰
                enriched_paper = {**paper, **paper_data}
                enriched_papers.append(enriched_paper)
            
            return "\n\n".join(formatted_papers), enriched_papers
            
        except json.JSONDecodeError as e:
            print(f"JSON è§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å†…å®¹: {json_content[:500]}...")
            # å¦‚æœ JSON è§£æå¤±è´¥ï¼Œå°è¯•æå–å¹¶é‡è¯•
            # æŸ¥æ‰¾ JSON æ•°ç»„éƒ¨åˆ†
            json_match = re.search(r'\[.*\]', json_content, re.DOTALL)
            if json_match:
                try:
                    paper_data_list = json.loads(json_match.group(0))
                    return self._format_papers_from_json(json_match.group(0), papers, start_index)
                except:
                    pass
            raise
        except Exception as e:
            print(f"æ ¼å¼åŒ–å¤±è´¥: {e}")
            raise

    def _process_batch(self, papers: List[Dict[str, Any]], start_index: int):
        """å¤„ç†ä¸€æ‰¹è®ºæ–‡ï¼Œè¿”å›æ ¼å¼åŒ–æ–‡æœ¬å’Œç»“æ„åŒ–æ•°æ®
        
        Returns:
            tuple: (summaries_text, paper_data_list)
        """
        print(f"æ­£åœ¨æ‰¹é‡å¤„ç† {len(papers)} ç¯‡è®ºæ–‡...")
        summaries_text, paper_data_list = self._generate_batch_summaries(papers, start_index)
        time.sleep(1) 
        return summaries_text, paper_data_list

    def _fix_batch_format(self, text: str, start_index: int, batch_size: int) -> str:
        """ä¿®æ­£æ‰¹æ¬¡æ ¼å¼ï¼ˆä¿ç•™ä½œä¸ºå…¼å®¹æ€§æ–¹æ³•ï¼Œç°åœ¨æ ¼å¼å·²ç”±ä»£ç æ§åˆ¶ï¼‰"""
        # ç”±äºç°åœ¨æ ¼å¼ç”±ä»£ç æ§åˆ¶ï¼Œè¿™ä¸ªæ–¹æ³•ä¸»è¦ç”¨äºæ¸…ç†å¯èƒ½çš„é—®é¢˜
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼ˆè¶…è¿‡2ä¸ªè¿ç»­ç©ºè¡Œï¼‰
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text.strip()

    def _generate_batch_summary(self, papers: List[Dict[str, Any]]):
        """ç”Ÿæˆæ‰€æœ‰è®ºæ–‡çš„æ‘˜è¦ï¼Œè¿”å›æ ¼å¼åŒ–æ–‡æœ¬å’Œç»“æ„åŒ–æ•°æ®
        
        Returns:
            tuple: (summaries_text, all_paper_data) - æ ¼å¼åŒ–æ–‡æœ¬å’Œæ‰€æœ‰è®ºæ–‡çš„ç»“æ„åŒ–æ•°æ®
        """
        all_summaries = []
        all_paper_data = []
        total_papers = len(papers)
        
        for i in range(0, total_papers, self.max_papers_per_batch):
            batch = papers[i:i + self.max_papers_per_batch]
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {i + 1} åˆ° {min(i + self.max_papers_per_batch, total_papers)} ç¯‡è®ºæ–‡...")
            batch_summary, batch_paper_data = self._process_batch(batch, i + 1)
            
            # åå¤„ç†ï¼šä¿®æ­£åºå·å’Œæ ¼å¼
            batch_summary = self._fix_batch_format(batch_summary, i + 1, len(batch))
            
            all_summaries.append(batch_summary)
            all_paper_data.extend(batch_paper_data)
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ï¼Œç¡®ä¿æ‰¹æ¬¡ä¹‹é—´æœ‰åˆ†éš”ç¬¦
        result = "\n\n".join(all_summaries)
        
        return result, all_paper_data

    def _generate_trend_analysis(self, papers: List[Dict[str, Any]], paper_data_list: List[Dict[str, Any]]) -> str:
        """
        ä½¿ç”¨ embedding èšç±»ç­›é€‰ä»£è¡¨æ€§è®ºæ–‡ï¼Œç„¶åç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š
        
        Args:
            papers: åŸå§‹è®ºæ–‡åˆ—è¡¨
            paper_data_list: åŒ…å« LLM ç”Ÿæˆçš„ summary ç­‰å­—æ®µçš„ç»“æ„åŒ–æ•°æ®
        """
        print("\n" + "="*60)
        print("å¼€å§‹åŸºäº Embedding èšç±»çš„è¶‹åŠ¿åˆ†æ")
        print("="*60)
        
        try:
            # 1. æå–æ‰€æœ‰è®ºæ–‡çš„ summary å­—æ®µç”¨äº embedding
            summaries = [paper_data.get('summary', '') for paper_data in paper_data_list]
            
            if not summaries or len(summaries) == 0:
                print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°è®ºæ–‡æ‘˜è¦ï¼Œä½¿ç”¨é™çº§ç­–ç•¥")
                return self._generate_trend_analysis_fallback(papers, paper_data_list)
            
            print(f"æå–äº† {len(summaries)} ç¯‡è®ºæ–‡çš„æ‘˜è¦")
            
            # 2. è·å– embeddings
            embeddings = get_embeddings(summaries)
            
            if not embeddings or len(embeddings) != len(summaries):
                print("è­¦å‘Šï¼šEmbedding è·å–å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥")
                return self._generate_trend_analysis_fallback(papers, paper_data_list)
            
            # 3. è¿›è¡Œèšç±»
            # æ ¹æ®é…ç½®é€‰æ‹©èšç±»æ–¹æ³•
            from config.settings import CLUSTERING_CONFIG
            clustering_method = CLUSTERING_CONFIG.get('method', 'dbscan')
            
            if clustering_method == 'kmeans':
                from src.clustering import cluster_papers_kmeans
                n_clusters = CLUSTERING_CONFIG.get('n_clusters', 4)
                labels = cluster_papers_kmeans(embeddings, n_clusters)
            else:
                labels = cluster_papers(embeddings)
            
            # 4. é€‰æ‹©ä»£è¡¨æ€§è®ºæ–‡
            representative_papers = select_representative_papers(paper_data_list, embeddings, labels)
            
            if not representative_papers:
                print("è­¦å‘Šï¼šæœªèƒ½é€‰æ‹©ä»£è¡¨æ€§è®ºæ–‡ï¼Œä½¿ç”¨é™çº§ç­–ç•¥")
                return self._generate_trend_analysis_fallback(papers, paper_data_list)
            
            print(f"\nä» {len(paper_data_list)} ç¯‡è®ºæ–‡ä¸­ç­›é€‰å‡º {len(representative_papers)} ç¯‡ä»£è¡¨æ€§è®ºæ–‡")
            
            # 5. æ„å»ºä»£è¡¨æ€§è®ºæ–‡çš„æ‘˜è¦æ–‡æœ¬ï¼ˆç”¨äº LLM åˆ†æï¼‰
            representative_summaries = []
            for paper in representative_papers:
                summary_text = f"""
æ ‡é¢˜ï¼š{paper.get('title', 'Unknown')}
å…³é”®è¯ï¼š{paper.get('keywords', '')}
æ ¸å¿ƒç—›ç‚¹ï¼š{paper.get('core_pain_point', '')}
æŠ€æœ¯åˆ›æ–°ï¼š{paper.get('technical_innovation', '')}
æ€»ç»“ï¼š{paper.get('summary', '')}
"""
                representative_summaries.append(summary_text.strip())
            
            summaries_for_analysis = "\n\n---\n\n".join(representative_summaries)
            
            # 6. è°ƒç”¨ LLM ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š
            analysis_prompt = f"""
ä½ æ˜¯ä¸€åç§‘æŠ€æƒ…æŠ¥åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯ä»Šæ—¥ Arxiv æ›´æ–°çš„å¤§æ¨¡å‹(LLM)é¢†åŸŸè®ºæ–‡ä¸­ï¼Œé€šè¿‡èšç±»ç®—æ³•ç­›é€‰å‡ºçš„ {len(representative_papers)} ç¯‡ä»£è¡¨æ€§è®ºæ–‡çš„è¯¦ç»†æ‘˜è¦ã€‚

è¿™äº›è®ºæ–‡å·²ç»è¿‡æ™ºèƒ½èšç±»ï¼Œä»£è¡¨äº†ä»Šæ—¥è®ºæ–‡çš„ä¸»è¦ç ”ç©¶æ–¹å‘ã€‚è¯·åŸºäºè¿™äº›æ‘˜è¦å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½è¶‹åŠ¿ç®€æŠ¥ã€‚

è¦æ±‚ï¼š
1. æ ¹æ®æ‘˜è¦ä¸­çš„"å…³é”®è¯"ã€"æ ¸å¿ƒç—›ç‚¹"ã€"æŠ€æœ¯åˆ›æ–°"ç­‰ä¿¡æ¯ï¼Œå°†è®ºæ–‡å½’çº³ä¸º 2-4 ä¸ªæ ¸å¿ƒç ”ç©¶çƒ­ç‚¹ï¼ˆå¦‚ï¼šRAGä¼˜åŒ–ã€å¤šæ¨¡æ€ã€æ¨ç†åŠ é€Ÿã€å®‰å…¨å¯¹é½ç­‰ï¼‰ã€‚
2. æ¯ä¸ªçƒ­ç‚¹ä¸‹ï¼Œå†™ä¸€å¥ç®€çŸ­çš„"èµ›é“è§‚å¯Ÿ"ï¼ˆè¯´æ˜è¯¥æ–¹å‘ä»Šå¤©çš„æŠ€æœ¯çªç ´ç‚¹æˆ–å…³æ³¨ç‚¹ï¼‰ã€‚
3. åˆ—å‡ºå±äºè¯¥çƒ­ç‚¹çš„æœ€å…·ä»£è¡¨æ€§çš„è®ºæ–‡æ ‡é¢˜ï¼ˆåªåˆ—æ ‡é¢˜ï¼‰ã€‚

è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹ Markdown æ ¼å¼è¾“å‡ºï¼š

## ğŸ“Š ä»Šæ—¥è¶‹åŠ¿é€Ÿè§ˆ (Trend Analysis)

### ğŸ”¥ [çƒ­ç‚¹æ–¹å‘åç§°ï¼Œä¾‹å¦‚ï¼šRAG æ£€ç´¢å¢å¼º]
> **èµ›é“è§‚å¯Ÿï¼š** (ä¸€å¥è¯æ¦‚æ‹¬è¯¥æ–¹å‘ä»Šå¤©çš„æŠ€æœ¯çªç ´ç‚¹æˆ–å…³æ³¨ç‚¹)
- (è®ºæ–‡æ ‡é¢˜1)
- (è®ºæ–‡æ ‡é¢˜2)

### ğŸ¤– [çƒ­ç‚¹æ–¹å‘åç§°2]
> **èµ›é“è§‚å¯Ÿï¼š** ...
- ...

---

å¾…åˆ†æçš„ä»£è¡¨æ€§è®ºæ–‡æ‘˜è¦ï¼š
{summaries_for_analysis}
"""
            
            print("\næ­£åœ¨è°ƒç”¨ LLM ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š...")
            response = self.client.chat_completion([{
                "role": "user",
                "content": analysis_prompt
            }])
            
            result = response["choices"][0]["message"]["content"].strip()
            print("è¶‹åŠ¿æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼")
            return result
            
        except Exception as e:
            print(f"èšç±»è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            print("ä½¿ç”¨é™çº§ç­–ç•¥...")
            return self._generate_trend_analysis_fallback(papers, paper_data_list)
    
    def _generate_trend_analysis_fallback(self, papers: List[Dict[str, Any]], paper_data_list: List[Dict[str, Any]]) -> str:
        """
        é™çº§ç­–ç•¥ï¼šå½“èšç±»å¤±è´¥æ—¶ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š
        """
        print("ä½¿ç”¨é™çº§ç­–ç•¥ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Šï¼ˆä¸ä½¿ç”¨èšç±»ï¼‰")
        
        # æ„å»ºæ‰€æœ‰è®ºæ–‡çš„æ‘˜è¦æ–‡æœ¬
        all_summaries = []
        for paper in paper_data_list[:15]:  # æœ€å¤šå–å‰15ç¯‡é¿å… token è¶…é™
            summary_text = f"""
æ ‡é¢˜ï¼š{paper.get('title', 'Unknown')}
å…³é”®è¯ï¼š{paper.get('keywords', '')}
æ ¸å¿ƒç—›ç‚¹ï¼š{paper.get('core_pain_point', '')}
æŠ€æœ¯åˆ›æ–°ï¼š{paper.get('technical_innovation', '')}
"""
            all_summaries.append(summary_text.strip())
        
        summaries_for_analysis = "\n\n---\n\n".join(all_summaries)
        
        analysis_prompt = f"""
ä½ æ˜¯ä¸€åç§‘æŠ€æƒ…æŠ¥åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯ä»Šæ—¥ Arxiv æ›´æ–°çš„ {len(paper_data_list)} ç¯‡å¤§æ¨¡å‹(LLM)é¢†åŸŸè®ºæ–‡çš„æ‘˜è¦ä¿¡æ¯ã€‚

è¯·åŸºäºè¿™äº›æ‘˜è¦å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½è¶‹åŠ¿ç®€æŠ¥ã€‚

è¦æ±‚ï¼š
1. æ ¹æ®æ‘˜è¦ä¸­çš„"å…³é”®è¯"ã€"æ ¸å¿ƒç—›ç‚¹"ã€"æŠ€æœ¯åˆ›æ–°"ç­‰ä¿¡æ¯ï¼Œå°†è®ºæ–‡å½’çº³ä¸º 2-4 ä¸ªæ ¸å¿ƒç ”ç©¶çƒ­ç‚¹ã€‚
2. æ¯ä¸ªçƒ­ç‚¹ä¸‹ï¼Œå†™ä¸€å¥ç®€çŸ­çš„"èµ›é“è§‚å¯Ÿ"ã€‚
3. åˆ—å‡ºå±äºè¯¥çƒ­ç‚¹çš„æœ€å…·ä»£è¡¨æ€§çš„è®ºæ–‡æ ‡é¢˜ã€‚

è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹ Markdown æ ¼å¼è¾“å‡ºï¼š

## ğŸ“Š ä»Šæ—¥è¶‹åŠ¿é€Ÿè§ˆ (Trend Analysis)

### ğŸ”¥ [çƒ­ç‚¹æ–¹å‘åç§°]
> **èµ›é“è§‚å¯Ÿï¼š** (ä¸€å¥è¯æ¦‚æ‹¬)
- (è®ºæ–‡æ ‡é¢˜1)
- (è®ºæ–‡æ ‡é¢˜2)

---

å¾…åˆ†æçš„è®ºæ–‡æ‘˜è¦ï¼š
{summaries_for_analysis}
"""
        
        try:
            response = self.client.chat_completion([{
                "role": "user",
                "content": analysis_prompt
            }])
            return response["choices"][0]["message"]["content"].strip()
        except Exception as e:
            print(f"é™çº§ç­–ç•¥ä¹Ÿå¤±è´¥: {e}")
            return "*(è¶‹åŠ¿åˆ†æç”Ÿæˆå¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä¸‹æ–¹è¯¦ç»†åˆ—è¡¨)*"

    def summarize_papers(self, papers: List[Dict[str, Any]], output_file: str) -> bool:
        try:
            print(f"å¼€å§‹ç”Ÿæˆè®ºæ–‡æ€»ç»“ï¼Œå…± {len(papers)} ç¯‡...")
            
            # 1. ç”Ÿæˆæ‰€æœ‰å•ç¯‡è®ºæ–‡çš„æ‘˜è¦ï¼ˆè¿”å›æ–‡æœ¬å’Œç»“æ„åŒ–æ•°æ®ï¼‰
            summaries, paper_data_list = self._generate_batch_summary(papers)
            
            # 2. åŸºäºèšç±»ç­›é€‰ä»£è¡¨æ€§è®ºæ–‡ï¼Œç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š
            trend_analysis = self._generate_trend_analysis(papers, paper_data_list)
            
            # 3. å¯¹è®ºæ–‡æŒ‰æ¨èåº¦å’Œèšç±»ä¿¡æ¯æ’åº
            sorted_paper_data = self._sort_papers_by_priority(paper_data_list)
            
            # 4. é‡æ–°ç”Ÿæˆæ’åºåçš„æ‘˜è¦æ–‡æœ¬
            sorted_summaries = self._regenerate_summaries_text(sorted_paper_data)
            
            # 5. ç»„åˆæœ€ç»ˆæŠ¥å‘Š
            markdown_content = self._generate_markdown(papers, sorted_summaries, trend_analysis)
            
            # ä¿å­˜æ–‡ä»¶
            output_md = str(Path(output_file).with_suffix('.md'))
            with open(output_md, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            print(f"Markdownæ–‡ä»¶å·²ä¿å­˜ï¼š{output_md}")
            
            return True
            
        except Exception as e:
            print(f"ä¸¥é‡é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _sort_papers_by_priority(self, paper_data_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æŒ‰æ¨èåº¦å’Œèšç±»ä¿¡æ¯å¯¹è®ºæ–‡æ’åº
        
        æ’åºä¼˜å…ˆçº§ï¼š
        1. æ¨èå†³ç­–ï¼ˆæ¨è > è¾¹ç¼˜å¯çœ‹ > ä¸æ¨èï¼‰
        2. èšç±»æ’åï¼ˆå¤§èšç±»ä¼˜å…ˆï¼‰
        3. åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»ï¼ˆè¶Šè¿‘è¶Šé å‰ï¼‰
        """
        print("\næ­£åœ¨æŒ‰æ¨èåº¦å’Œèšç±»ä¿¡æ¯å¯¹è®ºæ–‡æ’åº...")
        
        # å®šä¹‰æ¨èå†³ç­–çš„ä¼˜å…ˆçº§
        decision_priority = {
            'æ¨è': 0,
            'è¾¹ç¼˜å¯çœ‹': 1,
            'ä¸æ¨è': 2,
            'æœªè¯„ä¼°': 3
        }
        
        def sort_key(paper):
            decision = paper.get('decision', 'æœªè¯„ä¼°')
            cluster_rank = paper.get('_cluster_rank', 999)
            distance = paper.get('_distance_to_center', 999.0)
            
            return (
                decision_priority.get(decision, 999),  # æ¨èå†³ç­–ä¼˜å…ˆçº§
                cluster_rank,                          # èšç±»æ’åï¼ˆ0=æœ€å¤§èšç±»ï¼‰
                distance                               # åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»
            )
        
        sorted_papers = sorted(paper_data_list, key=sort_key)
        
        # æ‰“å°æ’åºç»“æœç»Ÿè®¡
        print(f"æ’åºå®Œæˆï¼š")
        for i, paper in enumerate(sorted_papers[:5], 1):
            decision = paper.get('decision', 'æœªè¯„ä¼°')
            cluster_id = paper.get('_cluster_id', 'N/A')
            cluster_size = paper.get('_cluster_size', 'N/A')
            title = paper.get('title', 'Unknown')[:50]
            print(f"  {i}. [{decision}] èšç±»{cluster_id}({cluster_size}ç¯‡) - {title}...")
        
        if len(sorted_papers) > 5:
            print(f"  ... è¿˜æœ‰ {len(sorted_papers) - 5} ç¯‡è®ºæ–‡")
        
        return sorted_papers

    def _regenerate_summaries_text(self, sorted_paper_data: List[Dict[str, Any]]) -> str:
        """
        æ ¹æ®æ’åºåçš„è®ºæ–‡æ•°æ®é‡æ–°ç”Ÿæˆæ‘˜è¦æ–‡æœ¬
        """
        print("æ­£åœ¨é‡æ–°ç”Ÿæˆæ’åºåçš„æ‘˜è¦æ–‡æœ¬...")
        
        formatted_papers = []
        
        for i, paper_data in enumerate(sorted_paper_data, 1):
            decision = paper_data.get('decision', 'æœªè¯„ä¼°')
            
            formatted_paper = f"""## {i}. {paper_data.get('title', 'Unknown')}
- **ä¸­æ–‡æ ‡é¢˜**: {paper_data.get('chinese_title', '')}
- **Link**: {paper_data.get('entry_id', '')}
- **æ¨èå†³ç­–:** {decision}
- **å†³ç­–ç†ç”±:** {paper_data.get('decision_reason', '')}
- **å…³é”®è¯:** {paper_data.get('keywords', '')}
- **æ ¸å¿ƒç—›ç‚¹:** {paper_data.get('core_pain_point', '')}
- **åº”ç”¨ä»·å€¼:** {paper_data.get('application_value', '')}
- **æ€»ç»“:** {paper_data.get('summary', '')}
- **æŠ€æœ¯åˆ›æ–°:** {paper_data.get('technical_innovation', '')}


---"""
            formatted_papers.append(formatted_paper)
        
        return "\n\n".join(formatted_papers)

    def _generate_markdown(self, papers: List[Dict[str, Any]], summaries: str, trend_analysis: str = "") -> str:
        beijing_time = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S')
        
        # ç»Ÿè®¡æ¨èå†³ç­–åˆ†å¸ƒ
        recommend_count = summaries.count('**æ¨èå†³ç­–:** æ¨è')
        maybe_count = summaries.count('**æ¨èå†³ç­–:** è¾¹ç¼˜å¯çœ‹')
        not_recommend_count = summaries.count('**æ¨èå†³ç­–:** ä¸æ¨è')
        
        return f"""# Arxiv LLM æ¯æ—¥ç ”æŠ¥

> **æ›´æ–°æ—¶é—´**ï¼š{beijing_time}
> **è®ºæ–‡æ•°é‡**ï¼š{len(papers)} ç¯‡
> **æ¨èåˆ†å¸ƒ**ï¼šâ­æ¨è {recommend_count} ç¯‡ | ğŸ“Œè¾¹ç¼˜å¯çœ‹ {maybe_count} ç¯‡ | âŒä¸æ¨è {not_recommend_count} ç¯‡
> **è‡ªåŠ¨ç”Ÿæˆ**ï¼šBy Arxiv_LLM_Daily Agent

---

{trend_analysis}

---

## ğŸ“ è®ºæ–‡è¯¦ç»†åˆ—è¡¨

{summaries}

---
*Generated by AI Agent based on arXiv (cs.CL)*
"""
def create_summarizer(api_key: str, model: Optional[str] = None):
    return PaperSummarizer(api_key, model)