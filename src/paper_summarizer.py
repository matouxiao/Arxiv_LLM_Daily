"""
è®ºæ–‡æ€»ç»“æ¨¡å— - ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹APIç”Ÿæˆè®ºæ–‡æ‘˜è¦
"""
import os
import json
import re
from typing import List, Dict, Any, Optional
from pathlib import Path
import requests
import time
from datetime import datetime
import pytz
import numpy as np
from config.settings import LLM_CONFIG
from src.clustering import get_embeddings, cluster_papers, select_representative_papers
from src.visualizer import generate_decision_pie_chart, generate_trend_pie_chart, generate_keywords_pie_chart

class ModelClient:
    """å…¼å®¹ OpenAI æ¥å£æ ¼å¼çš„ API å®¢æˆ·ç«¯ (é€‚é… DashScope/DeepSeek)"""
    
    def __init__(self, api_key: str, model: Optional[str] = None):
        self.api_key = api_key
        self.model = model or LLM_CONFIG['model']
        # è‡ªåŠ¨æ‹¼æ¥ chat/completions ç«¯ç‚¹
        base_url = LLM_CONFIG['api_url'].rstrip('/')
        self.api_url = f"{base_url}/chat/completions"
        self.timeout = LLM_CONFIG.get('timeout', 60)
        
    def _create_headers(self) -> Dict[str, str]:
        """åˆ›å»ºè¯·æ±‚å¤´"""
        return {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
    
    def _create_request_body(
        self, 
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºç¬¦åˆ OpenAI æ ¼å¼çš„è¯·æ±‚ä½“"""
        return {
            "model": self.model,
            "messages": messages,
            "temperature": temperature or LLM_CONFIG['temperature'],
            "max_tokens": max_tokens or LLM_CONFIG['max_output_tokens'],
            # DashScope/OpenAI é€šå¸¸ä½¿ç”¨ top_pï¼Œä¸ä¸€å®šéœ€è¦ top_kï¼Œè§†æ¨¡å‹è€Œå®š
            "top_p": LLM_CONFIG['top_p']
        }
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """å‘é€è¯·æ±‚å¹¶è·å–å›å¤"""
        headers = self._create_headers()
        data = self._create_request_body(messages, temperature, max_tokens)
        
        for attempt in range(LLM_CONFIG['retry_count']):
            try:
                # æ‰“å°è°ƒè¯•ä¿¡æ¯ (æ³¨æ„ä¸è¦æ‰“å°å®Œæ•´çš„ API Key)
                print(f"æ­£åœ¨è°ƒç”¨æ¨¡å‹: {self.model}...")
                
                response = requests.post(
                    self.api_url,
                    headers=headers,
                    json=data,
                    timeout=self.timeout
                )
                
                if response.status_code != 200:
                    raise Exception(f"API è°ƒç”¨å¤±è´¥ [{response.status_code}]: {response.text}")
                    
                result = response.json()
                
                # è§£æ OpenAI æ ¼å¼çš„å“åº”
                content = result["choices"][0]["message"]["content"]
                
                return {
                    "choices": [{
                        "message": {
                            "role": "assistant",
                            "content": content
                        },
                        "finish_reason": "stop"
                    }],
                    "usage": result.get("usage", {})
                }
                
            except requests.Timeout:
                print(f"è¯·æ±‚è¶…æ—¶ï¼ˆ{self.timeout}ç§’ï¼‰ï¼Œæ­£åœ¨é‡è¯• ({attempt + 1}/{LLM_CONFIG['retry_count']})...")
                time.sleep(LLM_CONFIG['retry_delay'])
            except Exception as e:
                print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
                if attempt == LLM_CONFIG['retry_count'] - 1:
                    raise
                time.sleep(LLM_CONFIG['retry_delay'])

class PaperSummarizer:
    def __init__(self, api_key: str, model: Optional[str] = None):
        self.client = ModelClient(api_key, model)
        # æ ¹æ® Token é™åˆ¶è°ƒæ•´æ‰¹å¤„ç†å¤§å°ï¼ŒDeepSeek/Qwen ä¸Šä¸‹æ–‡è¾ƒé•¿ï¼Œå¯ä»¥è®¾å¤§ä¸€ç‚¹
        self.max_papers_per_batch = 5 

    def _generate_batch_summaries(self, papers: List[Dict[str, Any]], start_index: int) -> str:
        """ä¸ºä¸€æ‰¹è®ºæ–‡ç”Ÿæˆæ€»ç»“"""
        batch_prompt = ""
        has_full_text = False
        
        for i, paper in enumerate(papers, start=start_index):
            # ä¼˜å…ˆä½¿ç”¨å…³é”®ç« èŠ‚ï¼ˆæ‘˜è¦ã€Introductionã€Related Workã€Conclusionï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ‘˜è¦
            paper_content = paper.get('full_text')
            if paper_content:
                has_full_text = True
                content_type = "æ‘˜è¦ã€ä»‹ç»ã€ç›¸å…³å·¥ä½œå’Œæ€»ç»“"
                # é™åˆ¶é•¿åº¦é¿å… token è¶…é™ï¼ˆä¿ç•™å‰ 20000 å­—ç¬¦ï¼Œé€šå¸¸è¶³å¤Ÿè¦†ç›–å…³é”®ç« èŠ‚ï¼‰
                if len(paper_content) > 20000:
                    paper_content = paper_content[:20000] + "\n\n[æ³¨ï¼šå†…å®¹å·²æˆªæ–­ï¼Œä»…æ˜¾ç¤ºå‰20000å­—ç¬¦]"
            else:
                content_type = "æ‘˜è¦"
                paper_content = paper.get('summary', 'æ— æ‘˜è¦')
            
            batch_prompt += f"""
è®ºæ–‡ {i}ï¼š
æ ‡é¢˜ï¼š{paper['title']}
ä½œè€…ï¼š{', '.join(paper['authors'])}
å‘å¸ƒæ—¥æœŸï¼š{paper['published'][:10]}
arXivé“¾æ¥ï¼š{paper['entry_id']}
{content_type}ï¼š
{paper_content}

"""
        
        content_desc = "æ‘˜è¦ã€ä»‹ç»ã€ç›¸å…³å·¥ä½œå’Œæ€»ç»“" if has_full_text else "æ‘˜è¦"
        # è®©æ¨¡å‹åªç”Ÿæˆç»“æ„åŒ–å†…å®¹ï¼ˆJSONæ ¼å¼ï¼‰ï¼Œæ ¼å¼ç”±ä»£ç æ§åˆ¶
        final_prompt = f"""ä½ æ˜¯ä¸€åè´Ÿè´£"é‡‘èé¢†åŸŸ AI åº”ç”¨ç ”ç©¶"çš„é«˜çº§æŠ€æœ¯è¯„å®¡ä¸“å®¶ã€‚

ã€ä»»åŠ¡ã€‘
å¯¹ç»™å®šçš„è®ºæ–‡è¿›è¡Œç­›é€‰åˆ¤æ–­ï¼Œè¯„ä¼°å…¶æ˜¯å¦å€¼å¾—è¿›å…¥"é‡ç‚¹ç²¾è¯» / å¤ç° / åº”ç”¨è¯„ä¼°"æ± ã€‚

ä½ çš„ç›®æ ‡ä¸æ˜¯åˆ¤æ–­è®ºæ–‡æ˜¯å¦å­¦æœ¯ä¸Šä¸¥è°¨ï¼Œè€Œæ˜¯åˆ¤æ–­è¯¥è®ºæ–‡æ˜¯å¦ï¼š
- å±äºæˆ‘å…³æ³¨çš„æŠ€æœ¯æ–¹å‘
- å…·æœ‰æ˜ç¡®çš„å·¥ç¨‹å®ç°æˆ–åº”ç”¨ä»·å€¼
- å…·å¤‡è¿ç§»æˆ–æ”¹é€ åˆ°é‡‘èåœºæ™¯çš„å¯è¡Œæ€§

ã€é‡ç‚¹å…³æ³¨çš„ç ”ç©¶æ–¹å‘ï¼ˆè‡³å°‘å‘½ä¸­å…¶ä¸€ï¼‰ã€‘

1. éŸ³é¢‘ç›¸å…³ï¼ˆSpeech / Audioï¼‰
   - è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³ç†è§£ã€éŸ³é¢‘ç”Ÿæˆ
   - éŸ³é¢‘ä¸æ–‡æœ¬/è§†è§‰/ç»“æ„åŒ–æ•°æ®çš„å¤šæ¨¡æ€èåˆ
   - éŸ³é¢‘åœ¨ä¸šåŠ¡åœºæ™¯ä¸­çš„åº”ç”¨ï¼ˆå¦‚é€šè¯åˆ†æã€é£æ§ã€å®¢æœã€åˆè§„ï¼‰

2. å¤šæ¨¡æ€ï¼ˆMultimodalï¼‰
   - æ–‡æœ¬-å›¾åƒ / æ–‡æœ¬-éŸ³é¢‘ / æ–‡æœ¬-ç»“æ„åŒ–æ•°æ®
   - å¤šæ¨¡æ€æ¨ç†ã€å¤šæ¨¡æ€å¯¹é½ã€å¤šæ¨¡æ€æ•°æ®æ„é€ 
   - å¤šæ¨¡æ€åœ¨çœŸå®ä¸šåŠ¡ä»»åŠ¡ä¸­çš„è½åœ°æ–¹æ³•

3. å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰
   - LLM çš„å·¥ç¨‹åŒ–ã€æ¨ç†èƒ½åŠ›ã€å¯¹é½ã€è¯„æµ‹ã€è®­ç»ƒç­–ç•¥
   - é¢å‘å…·ä½“ä»»åŠ¡çš„ LLM åº”ç”¨ï¼Œè€Œéæ³›æ³›ç†è®ºåˆ†æ
   - LLM + ä¸šåŠ¡ç³»ç»Ÿ / æ•°æ® / å·¥å…· çš„ç»„åˆæ–¹å¼

4. æ•°æ®åˆæˆï¼ˆData Synthesis / Synthetic Dataï¼‰
   - åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•
   - åˆæˆæ•°æ®åœ¨è®­ç»ƒã€è¯„æµ‹ã€å¯¹é½ä¸­çš„å®é™…ä½œç”¨
   - æœ‰å¯å¤ç°æˆ–å¯è¿ç§»çš„æ•°æ®ç”Ÿæˆæµç¨‹

5. æ™ºèƒ½ä½“ï¼ˆAgentï¼‰
   - LLM Agentã€Tool-using Agentã€å¤šæ­¥å†³ç­– Agent
   - é¢å‘ä»»åŠ¡æ‰§è¡Œã€è§„åˆ’ã€æœç´¢ã€å®¡æŸ¥ç­‰åº”ç”¨
   - æ˜ç¡®çš„ç³»ç»Ÿæ¶æ„ï¼Œè€ŒéæŠ½è±¡å®šä¹‰

6. MoEï¼ˆMixture of Expertsï¼‰
   - MoE åœ¨è®­ç»ƒ/æ¨ç†/ç³»ç»Ÿå±‚é¢çš„è®¾è®¡
   - ä¸“å®¶è·¯ç”±ã€è´Ÿè½½å‡è¡¡ã€æ¨ç†åŠ é€Ÿç­‰å·¥ç¨‹é—®é¢˜
   - ä¸å®é™…ä¸šåŠ¡è´Ÿè½½ç›¸å…³çš„ MoE åº”ç”¨

ã€éœ€è¦æ˜ç¡®è¿‡æ»¤æ‰çš„è®ºæ–‡ç±»å‹ã€‘

è¯·ç›´æ¥åˆ¤å®šä¸º"ä¸æ¨è"ï¼Œè‹¥è®ºæ–‡ä¸»è¦å±äºä»¥ä¸‹æƒ…å†µä¹‹ä¸€ï¼š
- çº¯ç†è®ºåˆ†æï¼ˆå¦‚ä»…æœ‰æ•°å­¦æ¨å¯¼ã€å¤æ‚å®šç†è¯æ˜ï¼‰
- æ–¹æ³•é«˜åº¦æŠ½è±¡ï¼Œç¼ºä¹æ˜ç¡®ä»»åŠ¡æˆ–ç³»ç»Ÿè½ç‚¹
- æ— å®éªŒï¼Œæˆ–å®éªŒä»…ä¸º toy task / äººå·¥åˆæˆå°ä»»åŠ¡
- åº”ç”¨åœºæ™¯ä¸é‡‘èã€ä¼ä¸šçº§ç³»ç»Ÿå‡ ä¹æ— å…³è”ï¼Œä¸”è¿ç§»æˆæœ¬æé«˜
- å®Œå…¨èšç„¦äºè¯æ˜æœ€ä¼˜æ€§ã€æ”¶æ•›æ€§ã€å¤æ‚åº¦ç•Œé™ï¼Œè€Œéå¯ç”¨ç³»ç»Ÿ
- å’Œä¿¡æ¯å®‰å…¨ç›¸å…³çš„è®ºæ–‡

è¯·é˜…è¯»ä»¥ä¸‹{len(papers)}ç¯‡è®ºæ–‡çš„{content_desc}ï¼Œä¸ºæ¯ç¯‡è®ºæ–‡æå–å…³é”®ä¿¡æ¯ã€‚

è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¾“å‡ºï¼Œæ¯ç¯‡è®ºæ–‡ä¸€ä¸ªå¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- chinese_title: ä¸­æ–‡ç¿»è¯‘çš„æ ‡é¢˜
- keywords: 2-5ä¸ªå…³é”®è¯æ ‡ç­¾ï¼Œç”¨ä¸­æ–‡é€—å·åˆ†éš”ï¼ˆä¾‹å¦‚ï¼šRAGä¼˜åŒ–ã€å¤šæ¨¡æ€ã€Agentæ¶æ„ï¼‰
- core_pain_point: æ ¸å¿ƒç—›ç‚¹ï¼Œä¸€å¥è¯æ¦‚æ‹¬ç°æœ‰æŠ€æœ¯æœ‰ä»€ä¹ˆç¼ºé™·
- technical_innovation: æŠ€æœ¯åˆ›æ–°ï¼Œè¯¦ç»†æè¿°è®ºæ–‡Methodéƒ¨åˆ†çš„æ–¹æ³•ã€æŠ€æœ¯ã€ç®—æ³•å’Œæµç¨‹ã€‚è¦æ±‚ï¼š1) ä½¿ç”¨ç¼–å·åˆ—è¡¨æ ¼å¼ï¼ˆ1) 2) 3) ...ï¼‰ï¼Œæ¯ä¸ªç¼–å·æè¿°ä¸€ä¸ªå…·ä½“çš„æŠ€æœ¯ç‚¹ï¼›2) è¯¦ç»†è¯´æ˜æŠ€æœ¯æ–¹æ³•ã€ç®—æ³•åç§°ã€æ¶æ„è®¾è®¡ã€æ•°æ®å¤„ç†æµç¨‹ç­‰ï¼›3) å¦‚æœæ¶‰åŠæ•°æ®é›†ï¼Œè¯´æ˜æ•°æ®è§„æ¨¡å’Œç±»å‹ï¼ˆå¦‚ï¼š2739ä¸ªç¯å¢ƒã€11,270æ¡å¤šè½®å¯¹è¯æ•°æ®ï¼‰ï¼›4) å¦‚æœæ¶‰åŠå®éªŒï¼Œè¯´æ˜å…³é”®å®éªŒç»“æœï¼ˆå¦‚ï¼šåœ¨5ä¸ªæ¨ç†åŸºå‡†ä¸Šè¶…è¶Šé™æ€æ•°æ®é›†è®­ç»ƒï¼‰ï¼›5) ç”¨ç®€æ´çš„æŠ€æœ¯è¯­è¨€æè¿°ï¼Œé¿å…å­¦æœ¯å¥—è¯ï¼›6) æ§åˆ¶åœ¨200å­—ä»¥å†…ï¼Œç¡®ä¿ä¿¡æ¯å®Œæ•´
- application_value: åº”ç”¨ä»·å€¼ï¼Œè¯´æ˜è¿™é¡¹æŠ€æœ¯èƒ½å¸¦æ¥ä»€ä¹ˆï¼ˆé™åˆ¶åœ¨88å­—ä»¥å†…ï¼‰
- summary: è¯¦ç»†æ€»ç»“ï¼Œæ§åˆ¶åœ¨200å­—ä»¥å†…
- decision: æ¨èå†³ç­–ï¼Œåªèƒ½æ˜¯ä»¥ä¸‹ä¸‰ä¸ªé€‰é¡¹ä¹‹ä¸€ï¼š"æ¨è" æˆ– "è¾¹ç¼˜å¯çœ‹" æˆ– "ä¸æ¨è"
- decision_reason: å†³ç­–ç†ç”±ï¼Œç”¨ä¸è¶…è¿‡150å­—æ€»ç»“ä¸ºä»€ä¹ˆåšå‡ºè¯¥åˆ¤æ–­ï¼ˆéœ€æ˜ç¡®è¯´æ˜æ˜¯å¦å‘½ä¸­å…³æ³¨æ–¹å‘ã€å·¥ç¨‹ä»·å€¼ã€é‡‘èåœºæ™¯å¯è¡Œæ€§ï¼‰

è¦æ±‚ï¼š
1. **æ‹’ç»åºŸè¯**ï¼šæ‰€æœ‰æè¿°å¿…é¡»ç›´å‡»è¦å®³ï¼Œä¸è¦ä½¿ç”¨"æœ¬æ–‡æå‡ºäº†..."è¿™ç§å­¦æœ¯å¥—è¯ã€‚
2. **é€šä¿—æ˜“æ‡‚**ï¼šç”¨ä¸šç•Œé€šç”¨çš„æŠ€æœ¯è¯­è¨€ï¼Œè€Œä¸æ˜¯æ™¦æ¶©çš„æ•°å­¦æè¿°ã€‚
3. **æŠ€æœ¯åˆ›æ–°è¯¦ç»†åŒ–**ï¼štechnical_innovation å­—æ®µå¿…é¡»è¯¦ç»†æè¿°è®ºæ–‡Methodéƒ¨åˆ†çš„å†…å®¹ï¼Œä½¿ç”¨ç¼–å·åˆ—è¡¨æ ¼å¼ï¼Œæ¯ä¸ªæŠ€æœ¯ç‚¹éƒ½è¦è¯´æ˜å…·ä½“çš„æ–¹æ³•ã€ç®—æ³•ã€æµç¨‹ã€æ•°æ®è§„æ¨¡æˆ–å®éªŒç»“æœã€‚æ ¼å¼ç¤ºä¾‹ï¼š"1) ç¼–ç¨‹é—®é¢˜è‡ªåŠ¨è½¬åŒ–ä¸ºå¯éªŒè¯æ¨ç†ç¯å¢ƒï¼ˆ2739ä¸ªç¯å¢ƒï¼‰ï¼›2) åŠ¨æ€éš¾åº¦æ§åˆ¶å™¨ä¿æŒç›®æ ‡å‡†ç¡®ç‡ï¼›3) ç¯å¢ƒæ·˜æ±°æœºåˆ¶ç»´æŒå¤šæ ·æ€§ï¼›4) åœ¨5ä¸ªæ¨ç†åŸºå‡†ä¸Šè¶…è¶Šé™æ€æ•°æ®é›†è®­ç»ƒã€‚"
4. **ä¿æŒç®€çŸ­**ï¼šæ€»ç»“ç¯‡å¹…ä¸¥æ ¼æ§åˆ¶åœ¨ 200 å­—ä»¥å†…ï¼Œåº”ç”¨ä»·å€¼é™åˆ¶åœ¨88å­—ä»¥å†…ï¼ŒæŠ€æœ¯åˆ›æ–°é™åˆ¶åœ¨200å­—ä»¥å†…ï¼Œå†³ç­–ç†ç”±é™åˆ¶åœ¨150å­—ä»¥å†…ã€‚
5. **åªè¾“å‡º JSON**ï¼šä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ–é¢å¤–æ–‡å­—ï¼Œç¡®ä¿æ‰€æœ‰è¾“å‡ºç¬¦åˆä¸¥æ ¼çš„ JSON æ ¼å¼ã€‚
6. **ä¸¥æ ¼è½¬ä¹‰**ï¼šç¡®ä¿è¾“å‡ºçš„ JSON å­—ç¬¦ä¸²ä¸­ï¼Œæ‰€æœ‰å­—æ®µå€¼å†…çš„åŒå¼•å·å¿…é¡»ä½¿ç”¨åæ–œæ è½¬ä¹‰ï¼ˆå¦‚ \"å†…å®¹\"ï¼‰ï¼Œç¦æ­¢å‡ºç°åŸå§‹çš„æ¢è¡Œç¬¦ã€‚
7. **ç»“æ„å®Œæ•´**ï¼šè¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„ JSON æ•°ç»„ï¼Œç¡®ä¿æ¯ç¯‡è®ºæ–‡å¯¹è±¡ä¹‹é—´æœ‰é€—å·åˆ†éš”ã€‚
8. **å†³ç­–å‡†ç¡®**ï¼šdecision å­—æ®µåªèƒ½æ˜¯"æ¨è"ã€"è¾¹ç¼˜å¯çœ‹"ã€"ä¸æ¨è"ä¸‰è€…ä¹‹ä¸€ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
[
  {{
    "chinese_title": "ç¤ºä¾‹æ ‡é¢˜",
    "keywords": "å…³é”®è¯1ã€å…³é”®è¯2ã€å…³é”®è¯3",
    "core_pain_point": "ä¸€å¥è¯ç—›ç‚¹æè¿°",
    "application_value": "åº”ç”¨ä»·å€¼æè¿°",
    "summary": "é€šä¿—ç®€å•åœ°æ€»ç»“è¯¥è®ºæ–‡çš„å†…å®¹",
    "decision": "æ¨è",
    "decision_reason": "å‘½ä¸­LLMå·¥ç¨‹åŒ–æ–¹å‘ï¼Œæå‡ºçš„å‚æ•°é«˜æ•ˆæ–¹æ³•å…·æœ‰æ˜ç¡®å®ç°è·¯å¾„ï¼Œå¯è¿ç§»åˆ°é‡‘èæ¨¡å‹è®­ç»ƒåœºæ™¯é™ä½æˆæœ¬",
    "technical_innovation": "1) ç¼–ç¨‹é—®é¢˜è‡ªåŠ¨è½¬åŒ–ä¸ºå¯éªŒè¯æ¨ç†ç¯å¢ƒï¼ˆ2739ä¸ªç¯å¢ƒï¼‰ï¼›2) åŠ¨æ€éš¾åº¦æ§åˆ¶å™¨ä¿æŒç›®æ ‡å‡†ç¡®ç‡ï¼›3) ç¯å¢ƒæ·˜æ±°æœºåˆ¶ç»´æŒå¤šæ ·æ€§ï¼›4) åœ¨5ä¸ªæ¨ç†åŸºå‡†ä¸Šè¶…è¶Šé™æ€æ•°æ®é›†è®­ç»ƒ"
  }},
  ...
]

å¾…å¤„ç†è®ºæ–‡åˆ—è¡¨ï¼š
{batch_prompt}"""

        try:
            response = self.client.chat_completion([{
                "role": "user",
                "content": final_prompt
            }])
            content = response["choices"][0]["message"]["content"].strip()
            
            # è§£æ JSON å¹¶ç»„è£…æˆå›ºå®šæ ¼å¼
            return self._format_papers_from_json(content, papers, start_index)
        except Exception as e:
            print(f"æ‰¹å¤„ç†ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # ä¿®å¤ï¼šè¿”å›å…ƒç»„æ ¼å¼ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²
            error_text = f"**[æœ¬æ‰¹æ¬¡ç”Ÿæˆå¤±è´¥]** é”™è¯¯ä¿¡æ¯: {str(e)}"
            return error_text, []

    def _format_papers_from_json(self, json_content: str, papers: List[Dict[str, Any]], start_index: int):
        """ä» JSON å†…å®¹ç»„è£…æˆå›ºå®šæ ¼å¼çš„ Markdownï¼ŒåŒæ—¶è¿”å›ç»“æ„åŒ–æ•°æ®
        
        Returns:
            tuple: (formatted_text, paper_data_list) - æ ¼å¼åŒ–æ–‡æœ¬å’Œç»“æ„åŒ–æ•°æ®
        """
        try:
            # 1. åŸºç¡€æ¸…ç†
            json_content = json_content.strip()
            if json_content.startswith('```'):
                json_content = re.sub(r'^```(?:json)?\s*\n', '', json_content)
                json_content = re.sub(r'\n```\s*$', '', json_content)
        
            # 2. å°è¯•ç›´æ¥è§£æï¼ˆä¼˜å…ˆç­–ç•¥ï¼‰
            try:
                paper_data_list = json.loads(json_content)
            except json.JSONDecodeError as e1:
                # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æ¸…ç†åå†è§£æ
                print(f"ç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æ¸…ç†: {e1}")
                
                # æ”¹è¿›çš„ JSON æ¸…ç†ç­–ç•¥ï¼šåªæ¸…ç†å­—ç¬¦ä¸²å€¼å†…çš„æœªè½¬ä¹‰æ¢è¡Œç¬¦
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¹¶ä¿®å¤å­—ç¬¦ä¸²å€¼å†…çš„æ¢è¡Œç¬¦
                def fix_string_newlines(match):
                    """ä¿®å¤å­—ç¬¦ä¸²å€¼å†…çš„æœªè½¬ä¹‰æ¢è¡Œç¬¦"""
                    full_match = match.group(0)
                    # å¦‚æœå­—ç¬¦ä¸²å†…åŒ…å«æœªè½¬ä¹‰çš„æ¢è¡Œç¬¦ï¼Œæ›¿æ¢ä¸ºç©ºæ ¼
                    if '\n' in full_match and '\\n' not in full_match:
                        # å°†æœªè½¬ä¹‰çš„æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
                        fixed = full_match.replace('\n', ' ').replace('\r', ' ')
                        # åˆå¹¶å¤šä¸ªè¿ç»­ç©ºæ ¼
                        fixed = re.sub(r'\s+', ' ', fixed)
                        return fixed
                    return full_match
                
                # åŒ¹é… JSON å­—ç¬¦ä¸²å€¼ï¼ˆåŒå¼•å·å†…çš„å†…å®¹ï¼Œè€ƒè™‘è½¬ä¹‰ï¼‰
                # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å®Œæ•´çš„å­—ç¬¦ä¸²å€¼ï¼ŒåŒ…æ‹¬è½¬ä¹‰å­—ç¬¦
                json_content_cleaned = json_content
                
                # å…ˆå°è¯•ä¿®å¤å­—ç¬¦ä¸²å€¼å†…çš„æ¢è¡Œç¬¦
                # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•ï¼šé€å­—ç¬¦å¤„ç†
                result = []
                i = 0
                in_string = False
                escape_next = False
                
                while i < len(json_content):
                    char = json_content[i]
                    
                    if escape_next:
                        result.append(char)
                        escape_next = False
                        i += 1
                        continue
                    
                    if char == '\\':
                        result.append(char)
                        escape_next = True
                        i += 1
                        continue
                    
                    if char == '"':
                        in_string = not in_string
                        result.append(char)
                        i += 1
                        continue
                    
                    if in_string and (char == '\n' or char == '\r'):
                        # åœ¨å­—ç¬¦ä¸²å†…çš„æœªè½¬ä¹‰æ¢è¡Œç¬¦ï¼Œæ›¿æ¢ä¸ºç©ºæ ¼
                        result.append(' ')
                        i += 1
                        continue
                    
                    result.append(char)
                    i += 1
                
                json_content_cleaned = ''.join(result)
                
                # æ¸…ç† JSON ç»“æ„å¤–çš„å¤šä½™ç©ºç™½ï¼ˆå¯¹è±¡/æ•°ç»„ä¹‹é—´çš„æ¢è¡Œï¼‰
                json_content_cleaned = re.sub(r'\s*}\s*{', '},{', json_content_cleaned)
                json_content_cleaned = re.sub(r'\s*]\s*\[', '],[', json_content_cleaned)
                
                # å†æ¬¡å°è¯•è§£æ
                try:
                    paper_data_list = json.loads(json_content_cleaned)
                except json.JSONDecodeError as e2:
                    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•æå–æœ€å¤–å±‚çš„ [ ]
                    print(f"æ¸…ç†åä»è§£æå¤±è´¥: {e2}")
                    match = re.search(r'\[.*\]', json_content_cleaned, re.DOTALL)
                    if match:
                        try:
                            paper_data_list = json.loads(match.group(0))
                        except json.JSONDecodeError as e3:
                            # æœ€åå°è¯•ï¼šä½¿ç”¨ä¿®å¤æ–¹æ³•
                            print(f"æå–æ•°ç»„åä»è§£æå¤±è´¥: {e3}")
                            fixed_json = self._fix_json_common_errors(match.group(0))
                            paper_data_list = json.loads(fixed_json)
                    else:
                        raise e2
            
            # éªŒè¯è§£æç»“æœ
            if not isinstance(paper_data_list, list):
                raise ValueError(f"è§£æç»“æœä¸æ˜¯æ•°ç»„ï¼Œè€Œæ˜¯ {type(paper_data_list)}")
            
            if len(paper_data_list) != len(papers):
                print(f"è­¦å‘Šï¼šè§£æå‡ºçš„è®ºæ–‡æ•°é‡ ({len(paper_data_list)}) ä¸é¢„æœŸ ({len(papers)}) ä¸åŒ¹é…ï¼Œå°†ä½¿ç”¨å®é™…è§£æçš„æ•°é‡")
                # å¦‚æœè§£æå‡ºçš„æ•°é‡å°‘äºé¢„æœŸï¼Œåªå¤„ç†è§£æå‡ºçš„éƒ¨åˆ†
                # å¦‚æœè§£æå‡ºçš„æ•°é‡å¤šäºé¢„æœŸï¼Œåªå¤„ç†é¢„æœŸçš„éƒ¨åˆ†
                min_len = min(len(paper_data_list), len(papers))
                paper_data_list = paper_data_list[:min_len]
                papers = papers[:min_len]
            
            # ç»„è£…æˆå›ºå®šæ ¼å¼
            formatted_papers = []
            enriched_papers = []  # ä¿å­˜ç»“æ„åŒ–æ•°æ®
            
            for i, (paper, paper_data) in enumerate(zip(papers, paper_data_list)):
                paper_num = start_index + i
                
                # è·å–å†³ç­–ä¿¡æ¯
                decision = paper_data.get('decision', 'æœªè¯„ä¼°')
                
                # å¦‚æœæ˜¯æ¨èè®ºæ–‡ï¼Œåœ¨æ ‡é¢˜å‰æ·»åŠ â­å›¾æ ‡
                title_prefix = "â­ " if decision == 'æ¨è' else ""
                
                formatted_paper = f"""## {paper_num}. {title_prefix}{paper['title']}
- **ä¸­æ–‡æ ‡é¢˜**: {paper_data.get('chinese_title', '')}
- **Link**: {paper['entry_id']}
- **æ¨èå†³ç­–:** {decision}
- **å†³ç­–ç†ç”±:** {paper_data.get('decision_reason', '')}
- **å…³é”®è¯:** {paper_data.get('keywords', '')}
- **æ ¸å¿ƒç—›ç‚¹:** {paper_data.get('core_pain_point', '')}
- **åº”ç”¨ä»·å€¼:** {paper_data.get('application_value', '')}
- **æ€»ç»“:** {paper_data.get('summary', '')}
- **æŠ€æœ¯åˆ›æ–°:** {paper_data.get('technical_innovation', '')}

---"""
                formatted_papers.append(formatted_paper)
                
                # ä¿å­˜ç»“æ„åŒ–æ•°æ®ï¼ˆåˆå¹¶åŸå§‹è®ºæ–‡ä¿¡æ¯å’ŒLLMç”Ÿæˆçš„ä¿¡æ¯ï¼‰
                enriched_paper = {**paper, **paper_data}
                enriched_papers.append(enriched_paper)
            
            return "\n\n".join(formatted_papers), enriched_papers
            
        except json.JSONDecodeError as e:
            print(f"JSON è§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å†…å®¹å‰500å­—ç¬¦: {json_content[:500]}")
            if len(json_content) > 500:
                print(f"åŸå§‹å†…å®¹å500å­—ç¬¦: {json_content[-500:]}")
            # å°è¯•æ›´æ¿€è¿›çš„ä¿®å¤ç­–ç•¥
            json_match = re.search(r'\[.*\]', json_content, re.DOTALL)
            if json_match:
                try:
                    fixed_json = self._fix_json_common_errors(json_match.group(0))
                    paper_data_list = json.loads(fixed_json)
                    # å¦‚æœä¿®å¤æˆåŠŸï¼Œç»§ç»­å¤„ç†
                    formatted_papers = []
                    enriched_papers = []
                    for i, (paper, paper_data) in enumerate(zip(papers, paper_data_list)):
                        paper_num = start_index + i
                        decision = paper_data.get('decision', 'æœªè¯„ä¼°')
                        # å¦‚æœæ˜¯æ¨èè®ºæ–‡ï¼Œåœ¨æ ‡é¢˜å‰æ·»åŠ â­å›¾æ ‡
                        title_prefix = "â­ " if decision == 'æ¨è' else ""
                        formatted_paper = f"""## {paper_num}. {title_prefix}{paper['title']}
- **ä¸­æ–‡æ ‡é¢˜**: {paper_data.get('chinese_title', '')}
- **Link**: {paper['entry_id']}
- **æ¨èå†³ç­–:** {decision}
- **å†³ç­–ç†ç”±:** {paper_data.get('decision_reason', '')}
- **å…³é”®è¯:** {paper_data.get('keywords', '')}
- **æ ¸å¿ƒç—›ç‚¹:** {paper_data.get('core_pain_point', '')}
- **åº”ç”¨ä»·å€¼:** {paper_data.get('application_value', '')}
- **æ€»ç»“:** {paper_data.get('summary', '')}
- **æŠ€æœ¯åˆ›æ–°:** {paper_data.get('technical_innovation', '')}

---"""
                        formatted_papers.append(formatted_paper)
                        enriched_paper = {**paper, **paper_data}
                        enriched_papers.append(enriched_paper)
                    return "\n\n".join(formatted_papers), enriched_papers
                except Exception as e2:
                    print(f"ä¿®å¤åä»è§£æå¤±è´¥: {e2}")
                    import traceback
                    traceback.print_exc()
            # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
            raise
        except Exception as e:
            print(f"æ ¼å¼åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    def _fix_json_common_errors(self, json_str: str) -> str:
        """ä¿®å¤å¸¸è§çš„ JSON æ ¼å¼é”™è¯¯"""
        # ä¿®å¤æœªè½¬ä¹‰çš„æ¢è¡Œç¬¦ï¼ˆåœ¨å­—ç¬¦ä¸²å€¼å†…ï¼‰
        result = []
        i = 0
        in_string = False
        escape_next = False
        
        while i < len(json_str):
            char = json_str[i]
            
            if escape_next:
                result.append(char)
                escape_next = False
                i += 1
                continue
            
            if char == '\\':
                result.append(char)
                escape_next = True
                i += 1
                continue
            
            if char == '"' and not escape_next:
                in_string = not in_string
                result.append(char)
                i += 1
                continue
            
            if in_string and (char == '\n' or char == '\r'):
                # åœ¨å­—ç¬¦ä¸²å†…çš„æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
                result.append(' ')
                i += 1
                continue
            
            result.append(char)
            i += 1
        
        fixed_json = ''.join(result)
        
        # ä¿®å¤å¸¸è§çš„é€—å·é—®é¢˜
        fixed_json = re.sub(r',\s*}', '}', fixed_json)  # ç§»é™¤å¯¹è±¡æœ«å°¾å¤šä½™çš„é€—å·
        fixed_json = re.sub(r',\s*]', ']', fixed_json)  # ç§»é™¤æ•°ç»„æœ«å°¾å¤šä½™çš„é€—å·
        
        # ä¿®å¤æœªé—­åˆçš„å¼•å·ï¼ˆç®€å•å¤„ç†ï¼‰
        # ç»Ÿè®¡å¼•å·æ•°é‡ï¼Œå¦‚æœå¥‡æ•°åˆ™å°è¯•ä¿®å¤
        quote_count = fixed_json.count('"')
        if quote_count % 2 != 0:
            # å°è¯•åœ¨æœ«å°¾æ·»åŠ å¼•å·
            if not fixed_json.rstrip().endswith('"'):
                fixed_json = fixed_json.rstrip() + '"'
        
        return fixed_json

    def _process_batch(self, papers: List[Dict[str, Any]], start_index: int):
        """å¤„ç†ä¸€æ‰¹è®ºæ–‡ï¼Œè¿”å›æ ¼å¼åŒ–æ–‡æœ¬å’Œç»“æ„åŒ–æ•°æ®
        
        Returns:
            tuple: (summaries_text, paper_data_list) - å¦‚æœå¤±è´¥ï¼Œpaper_data_list ä¸ºç©ºåˆ—è¡¨
        """
        print(f"æ­£åœ¨æ‰¹é‡å¤„ç† {len(papers)} ç¯‡è®ºæ–‡...")
        try:
            summaries_text, paper_data_list = self._generate_batch_summaries(papers, start_index)
            time.sleep(1) 
            return summaries_text, paper_data_list
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼ˆç¬¬ {start_index} åˆ° {start_index + len(papers) - 1} ç¯‡ï¼‰ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡ç»§ç»­å¤„ç†: {e}")
            import traceback
            traceback.print_exc()
            # ç”Ÿæˆä¸€ä¸ªé”™è¯¯æç¤ºçš„æ‘˜è¦æ–‡æœ¬
            error_summary = f"""## âš ï¸ æ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼ˆç¬¬ {start_index} åˆ° {start_index + len(papers) - 1} ç¯‡ï¼‰

**é”™è¯¯ä¿¡æ¯**: {str(e)}

**å—å½±å“çš„è®ºæ–‡**:
"""
            for i, paper in enumerate(papers, start=start_index):
                error_summary += f"- {i}. [{paper.get('title', 'Unknown')}]({paper.get('entry_id', '#')})\n"
            
            error_summary += "\n---"
            return error_summary, []

    def _fix_batch_format(self, text: str, start_index: int, batch_size: int) -> str:
        """ä¿®æ­£æ‰¹æ¬¡æ ¼å¼ï¼ˆä¿ç•™ä½œä¸ºå…¼å®¹æ€§æ–¹æ³•ï¼Œç°åœ¨æ ¼å¼å·²ç”±ä»£ç æ§åˆ¶ï¼‰"""
        # ç”±äºç°åœ¨æ ¼å¼ç”±ä»£ç æ§åˆ¶ï¼Œè¿™ä¸ªæ–¹æ³•ä¸»è¦ç”¨äºæ¸…ç†å¯èƒ½çš„é—®é¢˜
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼ˆè¶…è¿‡2ä¸ªè¿ç»­ç©ºè¡Œï¼‰
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text.strip()

    def _generate_batch_summary(self, papers: List[Dict[str, Any]]):
        """ç”Ÿæˆæ‰€æœ‰è®ºæ–‡çš„æ‘˜è¦ï¼Œè¿”å›æ ¼å¼åŒ–æ–‡æœ¬å’Œç»“æ„åŒ–æ•°æ®
        
        Returns:
            tuple: (summaries_text, all_paper_data) - æ ¼å¼åŒ–æ–‡æœ¬å’Œæ‰€æœ‰è®ºæ–‡çš„ç»“æ„åŒ–æ•°æ®
        """
        all_summaries = []
        all_paper_data = []
        total_papers = len(papers)
        failed_batches = 0
        
        for i in range(0, total_papers, self.max_papers_per_batch):
            batch = papers[i:i + self.max_papers_per_batch]
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {i + 1} åˆ° {min(i + self.max_papers_per_batch, total_papers)} ç¯‡è®ºæ–‡...")
            
            try:
                batch_summary, batch_paper_data = self._process_batch(batch, i + 1)
                
                # æ£€æŸ¥æ˜¯å¦å¤„ç†æˆåŠŸï¼ˆé€šè¿‡æ£€æŸ¥ paper_data_list æ˜¯å¦ä¸ºç©ºæ¥åˆ¤æ–­ï¼‰
                if not batch_paper_data:
                    # å¦‚æœè¿”å›ç©ºåˆ—è¡¨ï¼Œè¯´æ˜å¤„ç†å¤±è´¥ï¼Œä½†å·²ç»æœ‰é”™è¯¯ä¿¡æ¯äº†
                    failed_batches += 1
                    print(f"âš ï¸ è¯¥æ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼Œå·²è·³è¿‡")
                else:
                    # åå¤„ç†ï¼šä¿®æ­£åºå·å’Œæ ¼å¼
                    batch_summary = self._fix_batch_format(batch_summary, i + 1, len(batch))
                
                all_summaries.append(batch_summary)
                all_paper_data.extend(batch_paper_data)
                
            except Exception as e:
                # é¢å¤–çš„ä¿æŠ¤å±‚ï¼Œé˜²æ­¢æœªæ•è·çš„å¼‚å¸¸
                failed_batches += 1
                print(f"âŒ æ‰¹æ¬¡å¤„ç†å‡ºç°æœªæ•è·çš„å¼‚å¸¸ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡: {e}")
                import traceback
                traceback.print_exc()
                
                # ç”Ÿæˆé”™è¯¯æ‘˜è¦
                error_summary = f"""## âš ï¸ æ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼ˆç¬¬ {i + 1} åˆ° {min(i + self.max_papers_per_batch, total_papers)} ç¯‡ï¼‰

**é”™è¯¯ä¿¡æ¯**: {str(e)}

**å—å½±å“çš„è®ºæ–‡**:
"""
                for j, paper in enumerate(batch, start=i + 1):
                    error_summary += f"- {j}. [{paper.get('title', 'Unknown')}]({paper.get('entry_id', '#')})\n"
                
                error_summary += "\n---"
                all_summaries.append(error_summary)
                # paper_data_list ä¿æŒä¸ºç©ºï¼Œä¸æ·»åŠ ä»»ä½•æ•°æ®
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ï¼Œç¡®ä¿æ‰¹æ¬¡ä¹‹é—´æœ‰åˆ†éš”ç¬¦
        result = "\n\n".join(all_summaries)
        
        if failed_batches > 0:
            print(f"\nâš ï¸ è­¦å‘Šï¼šå…±æœ‰ {failed_batches} ä¸ªæ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼Œå·²è·³è¿‡")
            print(f"âœ… æˆåŠŸå¤„ç†äº† {len(all_paper_data)} ç¯‡è®ºæ–‡")
        
        return result, all_paper_data

    def _generate_trend_analysis(self, papers: List[Dict[str, Any]], paper_data_list: List[Dict[str, Any]]) -> tuple:
        """
        ä½¿ç”¨ embedding èšç±»ç­›é€‰ä»£è¡¨æ€§è®ºæ–‡ï¼Œç„¶åç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š
        
        Args:
            papers: åŸå§‹è®ºæ–‡åˆ—è¡¨
            paper_data_list: åŒ…å« LLM ç”Ÿæˆçš„ summary ç­‰å­—æ®µçš„ç»“æ„åŒ–æ•°æ®
            
        Returns:
            tuple: (trend_analysis_text, labels, embeddings) - è¶‹åŠ¿åˆ†ææ–‡æœ¬ã€èšç±»æ ‡ç­¾ã€embeddings
        """
        print("\n" + "="*60)
        print("å¼€å§‹åŸºäº Embedding èšç±»çš„è¶‹åŠ¿åˆ†æ")
        print("="*60)
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            if not paper_data_list:
                print("è­¦å‘Šï¼šæ²¡æœ‰è®ºæ–‡æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆè¶‹åŠ¿åˆ†æ")
                return ("## ğŸ“Š ä»Šæ—¥è¶‹åŠ¿é€Ÿè§ˆ (Trend Analysis)\n\nâš ï¸ ç”±äºæ²¡æœ‰æˆåŠŸå¤„ç†çš„è®ºæ–‡ï¼Œæ— æ³•ç”Ÿæˆè¶‹åŠ¿åˆ†ææŠ¥å‘Šã€‚", None, None)
            
            # 1. æå–æ‰€æœ‰è®ºæ–‡çš„ summary å­—æ®µç”¨äº embedding
            summaries = [paper_data.get('summary', '') for paper_data in paper_data_list]
            
            if not summaries or len(summaries) == 0:
                print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°è®ºæ–‡æ‘˜è¦ï¼Œä½¿ç”¨é™çº§ç­–ç•¥")
                return (self._generate_trend_analysis_fallback(papers, paper_data_list), None, None)
            
            print(f"æå–äº† {len(summaries)} ç¯‡è®ºæ–‡çš„æ‘˜è¦")
            
            # 2. è·å– embeddings
            embeddings = get_embeddings(summaries)
            
            if not embeddings or len(embeddings) != len(summaries):
                print("è­¦å‘Šï¼šEmbedding è·å–å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥")
                return (self._generate_trend_analysis_fallback(papers, paper_data_list), None, None)
            
            # 3. è¿›è¡Œèšç±»
            # æ ¹æ®é…ç½®é€‰æ‹©èšç±»æ–¹æ³•
            from config.settings import CLUSTERING_CONFIG
            clustering_method = CLUSTERING_CONFIG.get('method', 'dbscan')
            
            if clustering_method == 'kmeans':
                from src.clustering import cluster_papers_kmeans
                n_clusters = CLUSTERING_CONFIG.get('n_clusters', 4)
                labels = cluster_papers_kmeans(embeddings, n_clusters)
            else:
                labels = cluster_papers(embeddings)
            
            # è®¡ç®—å®é™…çš„èšç±»æ•°é‡ï¼ˆæ’é™¤å™ªå£°ç‚¹-1ï¼‰
            actual_cluster_count = len(set(labels)) - (1 if -1 in labels else 0)
            print(f"å®é™…èšç±»æ•°é‡: {actual_cluster_count}")
            
            # 4. é€‰æ‹©ä»£è¡¨æ€§è®ºæ–‡
            representative_papers = select_representative_papers(paper_data_list, embeddings, labels)
            
            if not representative_papers:
                print("è­¦å‘Šï¼šæœªèƒ½é€‰æ‹©ä»£è¡¨æ€§è®ºæ–‡ï¼Œä½¿ç”¨é™çº§ç­–ç•¥")
                return (self._generate_trend_analysis_fallback(papers, paper_data_list), labels, embeddings)
            
            print(f"\nä» {len(paper_data_list)} ç¯‡è®ºæ–‡ä¸­ç­›é€‰å‡º {len(representative_papers)} ç¯‡ä»£è¡¨æ€§è®ºæ–‡")
            
            # 5. æ„å»ºä»£è¡¨æ€§è®ºæ–‡çš„æ‘˜è¦æ–‡æœ¬ï¼ˆç”¨äº LLM åˆ†æï¼‰
            # æŒ‰èšç±»æ’åæ’åºï¼Œå¹¶æŒ‰èšç±»åˆ†ç»„
            from collections import defaultdict
            cluster_groups = defaultdict(list)
            for paper in representative_papers:
                cluster_rank = paper.get('_cluster_rank', 999)
                cluster_id = paper.get('_cluster_id', -1)
                cluster_size = paper.get('_cluster_size', 0)
                if cluster_id != -1:  # æ’é™¤å™ªå£°ç‚¹
                    cluster_groups[cluster_rank].append({
                        'paper': paper,
                        'cluster_id': cluster_id,
                        'cluster_size': cluster_size
                    })
            
            # æŒ‰èšç±»æ’åæ’åºï¼ˆä»å°åˆ°å¤§ï¼Œå³ä»å¤§åˆ°å°ï¼‰
            sorted_cluster_ranks = sorted(cluster_groups.keys())
            
            # æ„å»ºæ¯ä¸ªèšç±»çš„æ‘˜è¦æ–‡æœ¬
            cluster_summaries = []
            cluster_info = []  # ä¿å­˜èšç±»ä¿¡æ¯ç”¨äºprompt
            
            for rank in sorted_cluster_ranks:
                cluster_items = cluster_groups[rank]
                if not cluster_items:
                    continue
                
                # è·å–èšç±»ä¿¡æ¯ï¼ˆæ‰€æœ‰è®ºæ–‡çš„èšç±»ä¿¡æ¯åº”è¯¥ç›¸åŒï¼‰
                cluster_id = cluster_items[0]['cluster_id']
                cluster_size = cluster_items[0]['cluster_size']
                
                cluster_info.append({
                    'rank': rank,
                    'size': cluster_size,
                    'id': cluster_id
                })
                
                # æ„å»ºè¯¥èšç±»çš„è®ºæ–‡æ‘˜è¦
                cluster_paper_summaries = []
                for item in cluster_items:
                    paper = item['paper']
                    summary_text = f"""
æ ‡é¢˜ï¼š{paper.get('title', 'Unknown')}
å…³é”®è¯ï¼š{paper.get('keywords', '')}
æ ¸å¿ƒç—›ç‚¹ï¼š{paper.get('core_pain_point', '')}
æŠ€æœ¯åˆ›æ–°ï¼š{paper.get('technical_innovation', '')}
æ€»ç»“ï¼š{paper.get('summary', '')}
"""
                    cluster_paper_summaries.append(summary_text.strip())
                
                cluster_summary = f"""
ã€èšç±» {rank + 1}ã€‘ï¼ˆåŒ…å« {cluster_size} ç¯‡è®ºæ–‡ï¼ŒæŒ‰å¤§å°æ’åç¬¬ {rank + 1}ï¼‰
{chr(10).join(cluster_paper_summaries)}
"""
                cluster_summaries.append(cluster_summary.strip())
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„èšç±»ä¿¡æ¯
            if not cluster_info:
                print("è­¦å‘Šï¼šæ²¡æœ‰æœ‰æ•ˆçš„èšç±»ä¿¡æ¯ï¼Œä½¿ç”¨é™çº§ç­–ç•¥")
                return (self._generate_trend_analysis_fallback(papers, paper_data_list), labels, embeddings)
            
            # ç¡®ä¿ cluster_info çš„æ•°é‡ä¸ actual_cluster_count ä¸€è‡´
            if len(cluster_info) != actual_cluster_count:
                print(f"è­¦å‘Šï¼šèšç±»ä¿¡æ¯æ•°é‡({len(cluster_info)})ä¸èšç±»æ•°é‡({actual_cluster_count})ä¸ä¸€è‡´ï¼Œä½¿ç”¨å®é™…èšç±»ä¿¡æ¯æ•°é‡")
                actual_cluster_count = len(cluster_info)
            
            summaries_for_analysis = "\n\n" + "="*60 + "\n\n".join(cluster_summaries)
            
            # æ„å»ºèšç±»å¤§å°ä¿¡æ¯å­—ç¬¦ä¸²
            cluster_size_info = "\n".join([
                f"- èšç±» {i+1}ï¼ˆæ’åç¬¬ {i+1}ï¼ŒåŒ…å« {info['size']} ç¯‡è®ºæ–‡ï¼‰"
                for i, info in enumerate(cluster_info)
            ])
            
            # æ„å»ºæ ¼å¼ç¤ºä¾‹ä¸­çš„èšç±»å¤§å°ä¿¡æ¯ï¼ˆç”¨äºpromptä¸­çš„ç¤ºä¾‹ï¼‰
            def get_cluster_size_example(index):
                if index < len(cluster_info):
                    return f"{cluster_info[index]['size']}"
                return 'N/A'
            
            # 6. è°ƒç”¨ LLM ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š
            analysis_prompt = f"""
ä½ æ˜¯ä¸€åç§‘æŠ€æƒ…æŠ¥åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯ä»Šæ—¥ Arxiv æ›´æ–°çš„å¤§æ¨¡å‹(LLM)é¢†åŸŸè®ºæ–‡ä¸­ï¼Œé€šè¿‡èšç±»ç®—æ³•ç­›é€‰å‡ºçš„ä»£è¡¨æ€§è®ºæ–‡çš„è¯¦ç»†æ‘˜è¦ã€‚

è¿™äº›è®ºæ–‡å·²ç»è¿‡æ™ºèƒ½èšç±»ï¼Œå…±åˆ†ä¸º **{actual_cluster_count} ä¸ªç ”ç©¶çƒ­ç‚¹**ã€‚æ¯ä¸ªèšç±»çš„è®ºæ–‡æ•°é‡å¦‚ä¸‹ï¼ˆæŒ‰å¤§å°ä»å¤§åˆ°å°æ’åºï¼‰ï¼š

{cluster_size_info}

**é‡è¦è¦æ±‚ï¼š**
1. å¿…é¡»ç”Ÿæˆ **æ°å¥½ {actual_cluster_count} ä¸ª**æ ¸å¿ƒç ”ç©¶çƒ­ç‚¹ï¼Œä¸èƒ½å¤šä¹Ÿä¸èƒ½å°‘ã€‚
2. **å¿…é¡»æŒ‰ç…§èšç±»å¤§å°ä»å¤§åˆ°å°æ’åº**ï¼šç¬¬ä¸€ä¸ªçƒ­ç‚¹å¯¹åº”æœ€å¤§çš„èšç±»ï¼ˆ{get_cluster_size_example(0)} ç¯‡è®ºæ–‡ï¼‰ï¼Œç¬¬äºŒä¸ªçƒ­ç‚¹å¯¹åº”ç¬¬äºŒå¤§çš„èšç±»ï¼ˆ{get_cluster_size_example(1)} ç¯‡è®ºæ–‡ï¼‰ï¼Œä»¥æ­¤ç±»æ¨ã€‚
3. æ¯ä¸ªçƒ­ç‚¹å¿…é¡»å¯¹åº”ä¸€ä¸ªèšç±»ï¼Œä¸èƒ½åˆå¹¶å¤šä¸ªèšç±»ã€‚
4. æ ¹æ®æ‘˜è¦ä¸­çš„"å…³é”®è¯"ã€"æ ¸å¿ƒç—›ç‚¹"ã€"æŠ€æœ¯åˆ›æ–°"ç­‰ä¿¡æ¯ï¼Œä¸ºæ¯ä¸ªèšç±»å½’çº³ä¸€ä¸ªç ”ç©¶çƒ­ç‚¹åç§°ï¼ˆå¦‚ï¼šRAGä¼˜åŒ–ã€å¤šæ¨¡æ€ã€æ¨ç†åŠ é€Ÿã€å®‰å…¨å¯¹é½ç­‰ï¼‰ã€‚
5. æ¯ä¸ªçƒ­ç‚¹ä¸‹ï¼Œå†™ä¸€å¥ç®€çŸ­çš„"èµ›é“è§‚å¯Ÿ"ï¼ˆè¯´æ˜è¯¥æ–¹å‘ä»Šå¤©çš„æŠ€æœ¯çªç ´ç‚¹æˆ–å…³æ³¨ç‚¹ï¼‰ã€‚
6. åˆ—å‡ºå±äºè¯¥çƒ­ç‚¹çš„è®ºæ–‡æ ‡é¢˜ï¼ˆåªåˆ—æ ‡é¢˜ï¼Œè¿™äº›è®ºæ–‡æ¥è‡ªå¯¹åº”çš„èšç±»ï¼‰ã€‚

è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹ Markdown æ ¼å¼è¾“å‡ºï¼Œ**å¿…é¡»æŒ‰ç…§èšç±»å¤§å°ä»å¤§åˆ°å°æ’åº**ï¼š

## ğŸ“Š ä»Šæ—¥è¶‹åŠ¿é€Ÿè§ˆ (Trend Analysis)

### ğŸ”¥ [çƒ­ç‚¹æ–¹å‘åç§°1]ï¼ˆ{get_cluster_size_example(0)} ç¯‡è®ºæ–‡ï¼‰
> **èµ›é“è§‚å¯Ÿï¼š** (ä¸€å¥è¯æ¦‚æ‹¬è¯¥æ–¹å‘ä»Šå¤©çš„æŠ€æœ¯çªç ´ç‚¹æˆ–å…³æ³¨ç‚¹)
- (è®ºæ–‡æ ‡é¢˜1)
- (è®ºæ–‡æ ‡é¢˜2)

### ğŸ”¥ [çƒ­ç‚¹æ–¹å‘åç§°2]ï¼ˆ{get_cluster_size_example(1)} ç¯‡è®ºæ–‡ï¼‰
> **èµ›é“è§‚å¯Ÿï¼š** ...
- ...

### ğŸ”¥ [çƒ­ç‚¹æ–¹å‘åç§°3]ï¼ˆ{get_cluster_size_example(2)} ç¯‡è®ºæ–‡ï¼‰
> **èµ›é“è§‚å¯Ÿï¼š** ...
- ...

ï¼ˆç»§ç»­ç›´åˆ°ç”Ÿæˆ {actual_cluster_count} ä¸ªçƒ­ç‚¹ï¼Œä¸¥æ ¼æŒ‰ç…§èšç±»å¤§å°ä»å¤§åˆ°å°æ’åºï¼‰

---

å¾…åˆ†æçš„ä»£è¡¨æ€§è®ºæ–‡æ‘˜è¦ï¼ˆå·²æŒ‰èšç±»åˆ†ç»„ï¼‰ï¼š
{summaries_for_analysis}
"""
            
            print("\næ­£åœ¨è°ƒç”¨ LLM ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š...")
            response = self.client.chat_completion([{
                "role": "user",
                "content": analysis_prompt
            }])
            
            result = response["choices"][0]["message"]["content"].strip()
            print("è¶‹åŠ¿æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼")
            return (result, labels, embeddings)
            
        except Exception as e:
            print(f"èšç±»è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            print("ä½¿ç”¨é™çº§ç­–ç•¥...")
            return (self._generate_trend_analysis_fallback(papers, paper_data_list), None, None)
    
    def _generate_trend_analysis_fallback(self, papers: List[Dict[str, Any]], paper_data_list: List[Dict[str, Any]]) -> str:
        """
        é™çº§ç­–ç•¥ï¼šå½“èšç±»å¤±è´¥æ—¶ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š
        """
        print("ä½¿ç”¨é™çº§ç­–ç•¥ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Šï¼ˆä¸ä½¿ç”¨èšç±»ï¼‰")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if not paper_data_list:
            return "## ğŸ“Š ä»Šæ—¥è¶‹åŠ¿é€Ÿè§ˆ (Trend Analysis)\n\nâš ï¸ ç”±äºæ²¡æœ‰æˆåŠŸå¤„ç†çš„è®ºæ–‡ï¼Œæ— æ³•ç”Ÿæˆè¶‹åŠ¿åˆ†ææŠ¥å‘Šã€‚"
        
        # æ„å»ºæ‰€æœ‰è®ºæ–‡çš„æ‘˜è¦æ–‡æœ¬
        all_summaries = []
        for paper in paper_data_list[:15]:  # æœ€å¤šå–å‰15ç¯‡é¿å… token è¶…é™
            summary_text = f"""
æ ‡é¢˜ï¼š{paper.get('title', 'Unknown')}
å…³é”®è¯ï¼š{paper.get('keywords', '')}
æ ¸å¿ƒç—›ç‚¹ï¼š{paper.get('core_pain_point', '')}
æŠ€æœ¯åˆ›æ–°ï¼š{paper.get('technical_innovation', '')}
"""
            all_summaries.append(summary_text.strip())
        
        summaries_for_analysis = "\n\n---\n\n".join(all_summaries)
        
        analysis_prompt = f"""
ä½ æ˜¯ä¸€åç§‘æŠ€æƒ…æŠ¥åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯ä»Šæ—¥ Arxiv æ›´æ–°çš„ {len(paper_data_list)} ç¯‡å¤§æ¨¡å‹(LLM)é¢†åŸŸè®ºæ–‡çš„æ‘˜è¦ä¿¡æ¯ã€‚

è¯·åŸºäºè¿™äº›æ‘˜è¦å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½è¶‹åŠ¿ç®€æŠ¥ã€‚

è¦æ±‚ï¼š
1. æ ¹æ®æ‘˜è¦ä¸­çš„"å…³é”®è¯"ã€"æ ¸å¿ƒç—›ç‚¹"ã€"æŠ€æœ¯åˆ›æ–°"ç­‰ä¿¡æ¯ï¼Œå°†è®ºæ–‡å½’çº³ä¸º 2-4 ä¸ªæ ¸å¿ƒç ”ç©¶çƒ­ç‚¹ã€‚
2. æ¯ä¸ªçƒ­ç‚¹ä¸‹ï¼Œå†™ä¸€å¥ç®€çŸ­çš„"èµ›é“è§‚å¯Ÿ"ã€‚
3. åˆ—å‡ºå±äºè¯¥çƒ­ç‚¹çš„æœ€å…·ä»£è¡¨æ€§çš„è®ºæ–‡æ ‡é¢˜ã€‚

è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹ Markdown æ ¼å¼è¾“å‡ºï¼š

## ğŸ“Š ä»Šæ—¥è¶‹åŠ¿é€Ÿè§ˆ (Trend Analysis)

### ğŸ”¥ [çƒ­ç‚¹æ–¹å‘åç§°]
> **èµ›é“è§‚å¯Ÿï¼š** (ä¸€å¥è¯æ¦‚æ‹¬)
- (è®ºæ–‡æ ‡é¢˜1)
- (è®ºæ–‡æ ‡é¢˜2)

---

å¾…åˆ†æçš„è®ºæ–‡æ‘˜è¦ï¼š
{summaries_for_analysis}
"""
        
        try:
            response = self.client.chat_completion([{
                "role": "user",
                "content": analysis_prompt
            }])
            return response["choices"][0]["message"]["content"].strip()
        except Exception as e:
            print(f"é™çº§ç­–ç•¥ä¹Ÿå¤±è´¥: {e}")
            return "*(è¶‹åŠ¿åˆ†æç”Ÿæˆå¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä¸‹æ–¹è¯¦ç»†åˆ—è¡¨)*"

    def summarize_papers(self, papers: List[Dict[str, Any]], output_file: str) -> bool:
        try:
            print(f"å¼€å§‹ç”Ÿæˆè®ºæ–‡æ€»ç»“ï¼Œå…± {len(papers)} ç¯‡...")
            
            # 1. ç”Ÿæˆæ‰€æœ‰å•ç¯‡è®ºæ–‡çš„æ‘˜è¦ï¼ˆè¿”å›æ–‡æœ¬å’Œç»“æ„åŒ–æ•°æ®ï¼‰
            summaries, paper_data_list = self._generate_batch_summary(papers)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸå¤„ç†çš„è®ºæ–‡
            if not paper_data_list:
                print("âš ï¸ è­¦å‘Šï¼šæ‰€æœ‰æ‰¹æ¬¡å¤„ç†éƒ½å¤±è´¥äº†ï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´çš„æŠ¥å‘Š")
                # å³ä½¿å¤±è´¥ï¼Œä¹Ÿç”Ÿæˆä¸€ä¸ªåŒ…å«é”™è¯¯ä¿¡æ¯çš„æŠ¥å‘Š
                trend_analysis = "## âš ï¸ è¶‹åŠ¿åˆ†æ\n\nç”±äºæ‰€æœ‰æ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆè¶‹åŠ¿åˆ†ææŠ¥å‘Šã€‚"
                markdown_content = self._generate_markdown(papers, summaries, trend_analysis)
            else:
                # 2. åŸºäºèšç±»ç­›é€‰ä»£è¡¨æ€§è®ºæ–‡ï¼Œç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š
                labels = None
                embeddings = None
                try:
                    trend_analysis, labels, embeddings = self._generate_trend_analysis(papers, paper_data_list)
                except Exception as e:
                    print(f"âš ï¸ è¶‹åŠ¿åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥: {e}")
                    trend_analysis = self._generate_trend_analysis_fallback(papers, paper_data_list)
                
                # 3. å¯¹è®ºæ–‡æŒ‰æ¨èåº¦å’Œèšç±»ä¿¡æ¯æ’åº
                sorted_paper_data = self._sort_papers_by_priority(paper_data_list)
                
                # 4. é‡æ–°ç”Ÿæˆæ’åºåçš„æ‘˜è¦æ–‡æœ¬
                sorted_summaries = self._regenerate_summaries_text(sorted_paper_data)
                
                # 5. ç”Ÿæˆé¥¼å›¾ï¼ˆéœ€è¦trend_analysisæ¥æå–çƒ­ç‚¹æ ‡é¢˜ï¼‰
                pie_chart_paths = self._generate_pie_charts(
                    paper_data_list, 
                    labels, 
                    output_file,
                    trend_analysis=trend_analysis
                )
                
                # 6. æ›¿æ¢è¶‹åŠ¿åˆ†æä¸­çš„ç«ç„°å›¾æ ‡é¢œè‰²
                if 'trend_colors' in pie_chart_paths:
                    trend_analysis = self._replace_trend_icons_with_colors(
                        trend_analysis,
                        pie_chart_paths['trend_colors']
                    )
                
                # 7. ç»„åˆæœ€ç»ˆæŠ¥å‘Šï¼ˆä½¿ç”¨æ’åºåçš„æ‘˜è¦ï¼‰
                markdown_content = self._generate_markdown(
                    papers, 
                    sorted_summaries, 
                    trend_analysis,
                    pie_chart_paths
                )
            
            # ä¿å­˜æ–‡ä»¶
            output_md = str(Path(output_file).with_suffix('.md'))
            with open(output_md, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            print(f"Markdownæ–‡ä»¶å·²ä¿å­˜ï¼š{output_md}")
            
            return True
            
        except Exception as e:
            print(f"ä¸¥é‡é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _extract_trend_titles(self, trend_analysis: str) -> List[str]:
        """
        ä»è¶‹åŠ¿åˆ†ææ–‡æœ¬ä¸­æå–çƒ­ç‚¹æ ‡é¢˜
        
        Args:
            trend_analysis: è¶‹åŠ¿åˆ†ææ–‡æœ¬
            
        Returns:
            çƒ­ç‚¹æ ‡é¢˜åˆ—è¡¨ï¼ŒæŒ‰é¡ºåºæ’åˆ—
        """
        import re
        titles = []
        
        # åŒ¹é…æ ¼å¼ï¼š### <span ...> æ ‡é¢˜åç§°ï¼ˆå¯¹åº”...ï¼‰
        # æˆ–è€…ï¼š### ğŸ”¥ æ ‡é¢˜åç§°
        # æå–æ ‡é¢˜åç§°ï¼ˆåœ¨HTMLæ ‡ç­¾å’Œemojiä¹‹åï¼Œåœ¨"ï¼ˆå¯¹åº”"ä¹‹å‰ï¼‰
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å¼ï¼šåŒ¹é… ### åé¢çš„å†…å®¹ï¼Œç›´åˆ°é‡åˆ°"ï¼ˆå¯¹åº”"æˆ–è¡Œå°¾
        lines = trend_analysis.split('\n')
        
        for line in lines:
            # åŒ¹é… ### å¼€å¤´çš„è¡Œ
            if line.strip().startswith('###'):
                # ç§»é™¤ ### å’Œå¯èƒ½çš„HTMLæ ‡ç­¾
                content = re.sub(r'^###\s*', '', line)
                # ç§»é™¤HTML spanæ ‡ç­¾
                content = re.sub(r'<span[^>]*>.*?</span>', '', content)
                # ç§»é™¤emoji
                content = re.sub(r'[ğŸ”¥ğŸ¤–ğŸ§ ğŸš€ğŸŒâš–ï¸ğŸ“ŠğŸ› ï¸ğŸ’¡ğŸ¯âš¡ğŸŒŸâ­]+', '', content)
                # æå–æ ‡é¢˜ï¼ˆåœ¨"ï¼ˆå¯¹åº”"ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
                match = re.search(r'^([^ï¼ˆ]+)', content)
                if match:
                    title = match.group(1).strip()
                    # ç§»é™¤å¤šä½™ç©ºç™½
                    title = re.sub(r'\s+', ' ', title).strip()
                    if title:
                        titles.append(title)
        
        return titles
    
    def _generate_pie_charts(
        self,
        paper_data_list: List[Dict[str, Any]],
        labels: Optional[np.ndarray],
        output_file: str,
        trend_analysis: str = ""
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆè¶‹åŠ¿åˆ†å¸ƒé¥¼å›¾ï¼ˆä¸åŒ…æ‹¬æ¨èå†³ç­–åˆ†å¸ƒï¼‰
        
        Args:
            paper_data_list: è®ºæ–‡æ•°æ®åˆ—è¡¨
            labels: èšç±»æ ‡ç­¾æ•°ç»„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºç¡®å®šå›¾ç‰‡ä¿å­˜ä½ç½®ï¼‰
            trend_analysis: è¶‹åŠ¿åˆ†ææ–‡æœ¬ï¼ˆç”¨äºæå–çƒ­ç‚¹æ ‡é¢˜ï¼‰
            
        Returns:
            Dict[str, Any]: åŒ…å«é¥¼å›¾è·¯å¾„å’Œé¢œè‰²ä¿¡æ¯çš„å­—å…¸
        """
        pie_chart_paths = {}
        output_path = Path(output_file)
        base_dir = output_path.parent
        img_dir = base_dir / "img"
        img_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶åå‰ç¼€ï¼ˆåŸºäºè¾“å‡ºæ–‡ä»¶åï¼‰
        file_prefix = output_path.stem  # ä¾‹å¦‚: summary_20260115_113230
        
        try:
            # 1. ç”Ÿæˆç ”ç©¶çƒ­ç‚¹åˆ†å¸ƒé¥¼å›¾ï¼ˆå¦‚æœæœ‰èšç±»æ ‡ç­¾ï¼‰
            if labels is not None and len(labels) > 0:
                trend_chart_path = img_dir / f"{file_prefix}_trend_pie.png"
                
                # æå–çƒ­ç‚¹æ ‡é¢˜
                trend_titles = None
                if trend_analysis:
                    trend_titles = self._extract_trend_titles(trend_analysis)
                    if trend_titles:
                        print(f"æå–åˆ° {len(trend_titles)} ä¸ªçƒ­ç‚¹æ ‡é¢˜: {trend_titles}")
                
                trend_result = generate_trend_pie_chart(
                    paper_data_list,
                    labels,
                    str(trend_chart_path),
                    title="ç ”ç©¶çƒ­ç‚¹åˆ†å¸ƒ",
                    trend_titles=trend_titles
                )
                if trend_result and trend_result[0]:
                    pie_chart_paths['trend'] = f"img/{trend_chart_path.name}"
                    pie_chart_paths['trend_colors'] = trend_result[1]  # ä¿å­˜é¢œè‰²åˆ—è¡¨
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆç ”ç©¶çƒ­ç‚¹é¥¼å›¾å¤±è´¥: {e}")
        
        try:
            # 2. ç”Ÿæˆå…³é”®è¯åˆ†å¸ƒé¥¼å›¾
            keywords_chart_path = img_dir / f"{file_prefix}_keywords_pie.png"
            keywords_path = generate_keywords_pie_chart(
                paper_data_list,
                str(keywords_chart_path),
                top_n=8,
                title="å…³é”®è¯åˆ†å¸ƒï¼ˆTop 8ï¼‰"
            )
            if keywords_path:
                pie_chart_paths['keywords'] = f"img/{keywords_chart_path.name}"
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆå…³é”®è¯é¥¼å›¾å¤±è´¥: {e}")
        
        return pie_chart_paths

    def _replace_trend_icons_with_colors(self, trend_analysis: str, colors: List[str]) -> str:
        """
        å°†æ‰€æœ‰çƒ­ç‚¹æ–¹å‘çš„å›¾æ ‡ç»Ÿä¸€æ›¿æ¢ä¸ºåœ†å½¢å›¾æ ‡ï¼Œå¹¶ä½¿ç”¨é¥¼å›¾ä¸­çš„å¯¹åº”é¢œè‰²
        
        Args:
            trend_analysis: è¶‹åŠ¿åˆ†ææ–‡æœ¬
            colors: é¢œè‰²åˆ—è¡¨ï¼ˆåå…­è¿›åˆ¶æ ¼å¼ï¼‰ï¼ŒæŒ‰èšç±»å¤§å°æ’åº
            
        Returns:
            æ›¿æ¢åçš„æ–‡æœ¬
        """
        if not colors or not trend_analysis:
            return trend_analysis
        
        import re
        
        # æ‰©å±•emojiåŒ¹é…ï¼ŒåŒ…æ‹¬æ›´å¤šå¯èƒ½çš„emojiï¼ˆç¡®ä¿èƒ½åŒ¹é…åˆ°æ‰€æœ‰çƒ­ç‚¹ï¼‰
        # åŒ¹é…æ‰€æœ‰çƒ­ç‚¹æ–¹å‘çš„æ ‡é¢˜è¡Œï¼ˆ### åè·Ÿemojiå’Œæ–‡æœ¬ï¼‰
        # ä¾‹å¦‚: ### ğŸ”¥ [çƒ­ç‚¹æ–¹å‘åç§°] æˆ– ### ğŸ¤– [çƒ­ç‚¹æ–¹å‘åç§°]
        pattern = r'(###\s*)([ğŸ”¥ğŸ¤–ğŸ§ ğŸš€ğŸŒâš–ï¸ğŸ“ŠğŸ› ï¸ğŸ’¡ğŸ¯âš¡ğŸŒŸâ­]+)(\s+)'
        
        lines = trend_analysis.split('\n')
        icon_index = 0
        
        for i, line in enumerate(lines):
            match = re.search(pattern, line)
            if match:
                if icon_index < len(colors):
                    # åœ¨èšç±»æ•°é‡èŒƒå›´å†…ï¼Œåˆ†é…å¯¹åº”é¢œè‰²
                    color = colors[icon_index]
                    # ç»Ÿä¸€æ›¿æ¢ä¸ºåœ†å½¢å›¾æ ‡ï¼Œå¹¶ä½¿ç”¨å¯¹åº”é¢œè‰²
                    # ä½¿ç”¨HTML/CSSåˆ›å»ºåœ†å½¢å›¾æ ‡
                    circle_icon = f"<span style='display: inline-block; width: 12px; height: 12px; border-radius: 50%; background-color: {color}; margin-right: 6px; vertical-align: middle;'></span>"
                    replacement = f"{match.group(1)}{circle_icon}{match.group(3)}"
                    lines[i] = re.sub(pattern, replacement, line)
                    icon_index += 1
                else:
                    # å¦‚æœè¶…å‡ºèšç±»æ•°é‡ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œä½¿ç”¨é»˜è®¤ç°è‰²åœ†å½¢å›¾æ ‡
                    default_circle = "<span style='display: inline-block; width: 12px; height: 12px; border-radius: 50%; background-color: #999999; margin-right: 6px; vertical-align: middle;'></span>"
                    replacement = f"{match.group(1)}{default_circle}{match.group(3)}"
                    lines[i] = re.sub(pattern, replacement, line)
                    print(f"âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°è¶…å‡ºèšç±»æ•°é‡çš„çƒ­ç‚¹æ–¹å‘ï¼ˆç¬¬ {icon_index + 1} ä¸ªï¼‰ï¼Œå·²ä½¿ç”¨é»˜è®¤å›¾æ ‡")
        
        return '\n'.join(lines)
    
    def _sort_papers_by_priority(self, paper_data_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æŒ‰æ¨èåº¦å’Œèšç±»ä¿¡æ¯å¯¹è®ºæ–‡æ’åº
        
        æ’åºä¼˜å…ˆçº§ï¼š
        1. æ¨èå†³ç­–ï¼ˆæ¨è > è¾¹ç¼˜å¯çœ‹ > ä¸æ¨èï¼‰
        2. èšç±»æ’åï¼ˆå¤§èšç±»ä¼˜å…ˆï¼‰
        3. åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»ï¼ˆè¶Šè¿‘è¶Šé å‰ï¼‰
        """
        print("\næ­£åœ¨æŒ‰æ¨èåº¦å’Œèšç±»ä¿¡æ¯å¯¹è®ºæ–‡æ’åº...")
        
        # å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œç›´æ¥è¿”å›
        if not paper_data_list:
            print("âš ï¸ æ²¡æœ‰è®ºæ–‡æ•°æ®éœ€è¦æ’åº")
            return []
        
        # å®šä¹‰æ¨èå†³ç­–çš„ä¼˜å…ˆçº§
        decision_priority = {
            'æ¨è': 0,
            'è¾¹ç¼˜å¯çœ‹': 1,
            'ä¸æ¨è': 2,
            'æœªè¯„ä¼°': 3
        }
        
        def sort_key(paper):
            decision = paper.get('decision', 'æœªè¯„ä¼°')
            cluster_rank = paper.get('_cluster_rank', 999)
            distance = paper.get('_distance_to_center', 999.0)
            
            return (
                decision_priority.get(decision, 999),  # æ¨èå†³ç­–ä¼˜å…ˆçº§
                cluster_rank,                          # èšç±»æ’åï¼ˆ0=æœ€å¤§èšç±»ï¼‰
                distance                               # åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»
            )
        
        sorted_papers = sorted(paper_data_list, key=sort_key)
        
        # æ‰“å°æ’åºç»“æœç»Ÿè®¡
        print(f"æ’åºå®Œæˆï¼š")
        if sorted_papers:
            for i, paper in enumerate(sorted_papers[:5], 1):
                decision = paper.get('decision', 'æœªè¯„ä¼°')
                cluster_id = paper.get('_cluster_id', 'N/A')
                cluster_size = paper.get('_cluster_size', 'N/A')
                title = paper.get('title', 'Unknown')[:50]
                print(f"  {i}. [{decision}] èšç±»{cluster_id}({cluster_size}ç¯‡) - {title}...")
            
            if len(sorted_papers) > 5:
                print(f"  ... è¿˜æœ‰ {len(sorted_papers) - 5} ç¯‡è®ºæ–‡")
        
        return sorted_papers

    def _regenerate_summaries_text(self, sorted_paper_data: List[Dict[str, Any]]) -> str:
        """
        æ ¹æ®æ’åºåçš„è®ºæ–‡æ•°æ®é‡æ–°ç”Ÿæˆæ‘˜è¦æ–‡æœ¬
        """
        print("æ­£åœ¨é‡æ–°ç”Ÿæˆæ’åºåçš„æ‘˜è¦æ–‡æœ¬...")
        
        # å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›æç¤ºä¿¡æ¯
        if not sorted_paper_data:
            return "## âš ï¸ æ²¡æœ‰æˆåŠŸå¤„ç†çš„è®ºæ–‡\n\næ‰€æœ‰æ‰¹æ¬¡å¤„ç†éƒ½å¤±è´¥äº†ï¼Œè¯·æŸ¥çœ‹ä¸Šæ–¹çš„é”™è¯¯ä¿¡æ¯ã€‚"
        
        formatted_papers = []
        
        for i, paper_data in enumerate(sorted_paper_data, 1):
            decision = paper_data.get('decision', 'æœªè¯„ä¼°')
            
            # å¦‚æœæ˜¯æ¨èè®ºæ–‡ï¼Œåœ¨æ ‡é¢˜å‰æ·»åŠ â­å›¾æ ‡
            title_prefix = "â­ " if decision == 'æ¨è' else ""
            
            formatted_paper = f"""## {i}. {title_prefix}{paper_data.get('title', 'Unknown')}
- **ä¸­æ–‡æ ‡é¢˜**: {paper_data.get('chinese_title', '')}
- **Link**: {paper_data.get('entry_id', '')}
- **æ¨èå†³ç­–:** {decision}
- **å†³ç­–ç†ç”±:** {paper_data.get('decision_reason', '')}
- **å…³é”®è¯:** {paper_data.get('keywords', '')}
- **æ ¸å¿ƒç—›ç‚¹:** {paper_data.get('core_pain_point', '')}
- **åº”ç”¨ä»·å€¼:** {paper_data.get('application_value', '')}
- **æ€»ç»“:** {paper_data.get('summary', '')}
- **æŠ€æœ¯åˆ›æ–°:** {paper_data.get('technical_innovation', '')}


---"""
            formatted_papers.append(formatted_paper)
        
        return "\n\n".join(formatted_papers)

    def _generate_markdown(
        self, 
        papers: List[Dict[str, Any]], 
        summaries: str, 
        trend_analysis: str = "",
        pie_chart_paths: Dict[str, str] = None
    ) -> str:
        beijing_time = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S')
        
        # ç»Ÿè®¡æ¨èå†³ç­–åˆ†å¸ƒ
        recommend_count = summaries.count('**æ¨èå†³ç­–:** æ¨è')
        maybe_count = summaries.count('**æ¨èå†³ç­–:** è¾¹ç¼˜å¯çœ‹')
        not_recommend_count = summaries.count('**æ¨èå†³ç­–:** ä¸æ¨è')
        
        # æ„å»ºå¹¶æ’çš„é¥¼å›¾éƒ¨åˆ†
        pie_charts_section = ""
        if pie_chart_paths:
            # ä½¿ç”¨HTML divå®ç°å¹¶æ’æ˜¾ç¤º
            pie_charts_section = "\n\n<div style='display: flex; justify-content: space-around; align-items: flex-start; flex-wrap: wrap; gap: 20px; margin: 20px 0;'>\n\n"
            
            # ç ”ç©¶çƒ­ç‚¹åˆ†å¸ƒé¥¼å›¾
            if pie_chart_paths.get('trend'):
                pie_charts_section += f"<div style='flex: 1; min-width: 300px; text-align: center;'>\n"
                pie_charts_section += f"<h4 style='margin-bottom: 10px;'>ç ”ç©¶çƒ­ç‚¹åˆ†å¸ƒ</h4>\n"
                pie_charts_section += f"<img src='{pie_chart_paths['trend']}' alt='ç ”ç©¶çƒ­ç‚¹åˆ†å¸ƒ' style='max-width: 100%; height: auto;' />\n"
                pie_charts_section += f"</div>\n\n"
            
            # å…³é”®è¯åˆ†å¸ƒé¥¼å›¾
            if pie_chart_paths.get('keywords'):
                pie_charts_section += f"<div style='flex: 1; min-width: 300px; text-align: center;'>\n"
                pie_charts_section += f"<h4 style='margin-bottom: 10px;'>å…³é”®è¯åˆ†å¸ƒï¼ˆTop 8ï¼‰</h4>\n"
                pie_charts_section += f"<img src='{pie_chart_paths['keywords']}' alt='å…³é”®è¯åˆ†å¸ƒ' style='max-width: 100%; height: auto;' />\n"
                pie_charts_section += f"</div>\n\n"
            
            pie_charts_section += "</div>\n\n"
        
        # å°†é¥¼å›¾æ’å…¥åˆ°è¶‹åŠ¿åˆ†ææ ‡é¢˜ä¹‹åã€çƒ­ç‚¹æ–¹å‘ä¹‹å‰
        # trend_analysis æ ¼å¼é€šå¸¸æ˜¯: "## ğŸ“Š ä»Šæ—¥è¶‹åŠ¿é€Ÿè§ˆ (Trend Analysis)\n\n### ğŸ”¥ ..."
        if trend_analysis and pie_charts_section:
            lines = trend_analysis.split('\n')
            title_line_index = -1
            
            # æ‰¾åˆ°æ ‡é¢˜è¡Œ
            for i, line in enumerate(lines):
                if line.strip().startswith('## ğŸ“Š') or 'ä»Šæ—¥è¶‹åŠ¿é€Ÿè§ˆ' in line:
                    title_line_index = i
                    break
            
            if title_line_index >= 0:
                # æ‰¾åˆ°æ ‡é¢˜åçš„ç¬¬ä¸€ä¸ªç©ºè¡Œæˆ–å†…å®¹å¼€å§‹ä½ç½®
                insert_index = title_line_index + 1
                # è·³è¿‡æ ‡é¢˜åçš„ç©ºè¡Œ
                while insert_index < len(lines) and lines[insert_index].strip() == '':
                    insert_index += 1
                
                # åœ¨æ ‡é¢˜åã€å†…å®¹å‰æ’å…¥é¥¼å›¾
                trend_analysis = '\n'.join(lines[:insert_index]) + pie_charts_section + '\n'.join(lines[insert_index:])
            else:
                # å¦‚æœæ²¡æ‰¾åˆ°æ ‡é¢˜ï¼Œåœ¨å¼€å¤´æ’å…¥
                trend_analysis = pie_charts_section + trend_analysis
        
        return f"""# Arxiv LLM æ¯æ—¥ç ”æŠ¥

> **æ›´æ–°æ—¶é—´**ï¼š{beijing_time}
> **è®ºæ–‡æ•°é‡**ï¼š{len(papers)} ç¯‡
> **æ¨èåˆ†å¸ƒ**ï¼šâ­æ¨è {recommend_count} ç¯‡ | ğŸ“Œè¾¹ç¼˜å¯çœ‹ {maybe_count} ç¯‡ | âŒä¸æ¨è {not_recommend_count} ç¯‡
> **è‡ªåŠ¨ç”Ÿæˆ**ï¼šBy Arxiv_LLM_Daily Agent

---

{trend_analysis}

---

## ğŸ“ è®ºæ–‡è¯¦ç»†åˆ—è¡¨

{summaries}

---
*Generated by AI Agent based on arXiv (cs.CL)*
"""
def create_summarizer(api_key: str, model: Optional[str] = None):
    return PaperSummarizer(api_key, model)